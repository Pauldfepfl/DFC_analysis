{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tvb.simulator.lab as tsl\n",
    "import mne_connectivity \n",
    "import tvb\n",
    "from scipy import signal\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy \n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import pandas as pd\n",
    "import netplotbrain\n",
    "\n",
    "import functions as fn\n",
    "reload(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DFC_visu_pipeline(data, cor_method, num_clusters=3, window_size=2000, overlap=1000,tmax=30, intrahemispheric=False, square=[0,0],animated_visu=True,ts=0.5):\n",
    "    '''\n",
    "    This function performs visualisations regarding Dynamical Functional connectivity (DFC). \n",
    "\n",
    "    ------------\n",
    "    INPUTS\n",
    "    ------------\n",
    "\n",
    "    data : array_like, shape = (signal_duration/timestep,num_regions) \n",
    "        The signal data, typically eeg signal or source level signal. \n",
    "    cor_method : string\n",
    "        The correlation method to build the connectivity matrices. Can be chosen over 'Pearson', 'Spearman', 'Coherence', 'Phase-lock' and 'wpli'.\n",
    "    num_clusters : int, default = 3\n",
    "        Number of clusters for the k-means clustering.\n",
    "    window_size : int, default = 2000  \n",
    "        window_duration[ms]/timestep[ms]\n",
    "    overlap : int or float, default = 1000 \n",
    "        Time of overlap [ms] between two succint windows. \n",
    "    tmax : int, default = 30\n",
    "        Time of the simulation, for visualisation purpose. \n",
    "    intrahemispheric : bool, default = False\n",
    "        Visualisation of clustering of a single square of the connectivity matrix. \n",
    "    square : 2x2 list, default = [0,0]\n",
    "        If intrahemispheric = True, specifies the square of the matrix to perform the analysis on. \n",
    "    animated_visu : bool, default = True\n",
    "        Plot and save an animated plot showing the evolution of correlation matrices and pca visualisation. \n",
    "\n",
    "\n",
    "    ------------\n",
    "    OUTPUTS\n",
    "    ------------\n",
    "    closest_mats : array_like, shape = (num_clusters, num_regions,num_regions)\n",
    "        Closest connectivity matrix to every cluster. \n",
    "    most_repeating_states : array_like, shape = (num_clusters, num_regions,num_regions)\n",
    "         Kmeans clusters centroids. \n",
    "    correlation_matrices : array_like, shape = (num_windows, num_regions,num_regions)\n",
    "        Set of correlation matrics, one per window. \n",
    "    cluster_labels : list, len = num_clusters\n",
    "        Cluster labels.\n",
    "    distances : array_like, shape = (num_clusters, num_windows)\n",
    "         Matrix storing the distances between each connectivity matrix and each cluster. \n",
    "    pca : Sklearn PCA object.\n",
    "        PCA object used for the connectivity matrices dimension reduction.   \n",
    "    reduced_data : array_like\n",
    "        3D PCA reduced data.\n",
    "    '''\n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,num_clusters = fn.build_FCmat(data,cor_method,num_clusters,window_size,overlap,tmax,intrahemispheric,square=square)\n",
    "    distances = fn.compute_clst_dist(most_repeating_states, correlation_matrices, cluster_labels,tmax,num_clusters) \n",
    "    pca, reduced_data = fn.plot_PCA(correlation_matrices, most_repeating_states, cluster_labels,num_clusters)\n",
    "    if animated_visu : \n",
    "        fn.animated_plot(correlation_matrices,reduced_data,cluster_labels)\n",
    "\n",
    "    return closest_mats,most_repeating_states, correlation_matrices, cluster_labels, distances, pca, reduced_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects of window len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wins=[0.5,1,1.5,2,3,4,5,8,10] #s\n",
    "timestep = 0.5 #ms\n",
    "Wins=[i/timestep for i in Wins]\n",
    "# Number of clusters (most repeating states)\n",
    "num_clusters = 3\n",
    "mats=np.zeros((len(Wins), num_clusters,76,76))\n",
    "for i,w in enumerate(Wins):\n",
    "    data = np.load(f'{path_save}/test1.5minsim_dt_0.5_G_{7}_sigma_1e-07.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 80000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels, distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth,window_size=w*1000,overlap=w*500,tmax=t2/1000,animated_visu=False)\n",
    "        mats[i]=closest_mats\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 40 Minutes stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/20minsim_dt_0.5_G_{7}_sigma_1e-07.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 1200000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=10000,overlap=5000,tmax=t2/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipping hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D PCA\n",
    "num_clusters=3\n",
    "np.random.seed=42\n",
    "rmd_idx=np.random.randint(1,957,10)\n",
    "ext_data=np.concatenate((correlation_matrices,most_repeating_states))\n",
    "data=np.array(ext_data).reshape(len(correlation_matrices)+num_clusters,5776)\n",
    "#flipped_mtc=[inter_flip(mtx) for mtx in np.array(correlation_matrices)[rmd_idx]]\n",
    "flipped_mtc=[fn.intra_flip(mtx) for mtx in np.array(correlation_matrices)[rmd_idx]]\n",
    "pca_ref=PCA(n_components=3)\n",
    "reduced_data = pca_ref.fit_transform(data)\n",
    "flip_rshp = np.array(flipped_mtc).reshape(len(flipped_mtc),5776)\n",
    "flip_pca=pca_ref.transform(flip_rshp)\n",
    "centroids = reduced_data[-num_clusters:]\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(x=reduced_data[:-3, 0], y=reduced_data[:-3, 1], z=reduced_data[:-3, 2],\n",
    "                                mode='markers', marker=dict(color='black'), showlegend=False)])\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=reduced_data[rmd_idx, 0], y=reduced_data[rmd_idx, 1], z=reduced_data[rmd_idx, 2],\n",
    "                                mode='markers', marker=dict(color='red'), name='reference points'))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=flip_pca[:, 0], y=flip_pca[:, 1], z=flip_pca[:, 2],\n",
    "                                mode='markers', marker=dict(color='orange'), name='intrahemispheric flipped references'))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=centroids[:, 0], y=centroids[:, 1], z=centroids[:, 2],\n",
    "                                mode='markers+text', marker=dict(symbol='x', size=6),\n",
    "                            text=[str(i) for i in range(num_clusters)], name='Centroids', showlegend=False))\n",
    "for i in range(len(rmd_idx)):\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[reduced_data[rmd_idx[i], 0], flip_pca[i, 0]],\n",
    "        y=[reduced_data[rmd_idx[i], 1], flip_pca[i, 1]],\n",
    "        z=[reduced_data[rmd_idx[i], 2], flip_pca[i, 2]],\n",
    "        mode='lines',\n",
    "        line=dict(color='blue'),\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Centroids are marked with cross\\n\"\n",
    "    f\"Arrows indicate temporal order\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(correlation_matrices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fn.square_flip(correlation_matrices[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.components_\n",
    "for comp in components : \n",
    "    plt.figure()\n",
    "    plt.imshow(comp.reshape(76,76),cmap='bwr')\n",
    "    plt.colorbar();plt.clim([-0.05,0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distance to cluster + z-axis coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "distances = np.zeros((num_clusters,len(correlation_matrices)))\n",
    "time = np.linspace(1,2400000/1000,len(correlation_matrices))\n",
    "for j in range((num_clusters)):\n",
    "    for i in range(len(correlation_matrices)):\n",
    "        distances[j,i] = np.sum(np.abs(correlation_matrices[i]-most_repeating_states[j]))\n",
    "\n",
    "sums=np.sum(distances,axis=0)\n",
    "\n",
    "for i in range(len(correlation_matrices)):\n",
    "    distances[:,i] = 1-distances[:,i]/sums[i]\n",
    "\n",
    "colors=['red','blue','green','orange','black','purple']\n",
    "\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(time,savgol_filter(reduced_data[:-num_clusters, 2]/np.max(reduced_data[:, 2]), 40, 4),label='Smoothed relative PCA z-coordinate')\n",
    "for p in range((num_clusters)):\n",
    "    plt.plot(time,distances[p,:],label=str(p),c=colors[p])\n",
    "plt.title('1-Relative distance to each cluster over time')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('1 - Relative distance')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.scatter(time, cluster_labels, c = [colors[clust] for clust in cluster_labels], label=cluster_labels)\n",
    "plt.yticks(np.arange(0,num_clusters,1))\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('State')\n",
    "plt.title('Clustered state according to time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 min sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/20minsim_dt_0.5_G_{7}_sigma_1e-07.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 600000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=10000,overlap=5000,tmax=t2/1000,animated_visu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual cluster creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mat in most_repeating_states : \n",
    "    plt.figure()\n",
    "    plt.imshow(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D PCA\n",
    "num_clusters=3\n",
    "manual_clusters = [most_repeating_states[0],most_repeating_states[1],pca.inverse_transform([10,-5.5,0]).reshape(76,76),pca.inverse_transform([10,7,0]).reshape(76,76)]\n",
    "ext_data=np.concatenate((correlation_matrices,manual_clusters))\n",
    "data=np.array(ext_data).reshape(len(correlation_matrices)+4,5776)\n",
    "pca=PCA(n_components=3)\n",
    "reduced_data = pca.fit_transform(data)\n",
    "manual_clusters = [most_repeating_states[0],most_repeating_states[1],pca.inverse_transform([10,-5.5,0]).reshape(76,76),pca.inverse_transform([10,7,0]).reshape(76,76)]\n",
    "\n",
    "centroids = reduced_data[-4:,:]\n",
    "labels=['right','left','top','bottom']\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(x=reduced_data[:-4, 0], y=reduced_data[:-4, 1], z=reduced_data[:-4, 2],\n",
    "                                mode='markers', marker=dict(color='black', size=3))])\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=centroids[:, 0], y=centroids[:, 1], z=centroids[:, 2],\n",
    "                                mode='markers+text', marker=dict(symbol='x', size=7),\n",
    "                            text=[labels[i] for i in range(len(labels))], name='Centroids', showlegend=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Manually added Centroids are marked with cross\\n\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pca.inverse_transform([10,-5.5,0]).reshape(76,76))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "distances = np.zeros((4,len(correlation_matrices)))\n",
    "time = np.linspace(1,2400000/1000,len(correlation_matrices))\n",
    "for j in range((4)):\n",
    "    for i in range(len(correlation_matrices)):\n",
    "        distances[j,i] = np.sum(np.abs(correlation_matrices[i]-manual_clusters[j]))\n",
    "\n",
    "sums=np.sum(distances,axis=0)\n",
    "labels=['right','left','top','bottom']\n",
    "\n",
    "for i in range(len(correlation_matrices)):\n",
    "    distances[:,i] = 1-distances[:,i]/sums[i]\n",
    "colors=['red','blue','green','orange','black','purple']\n",
    "\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(time,savgol_filter(reduced_data[:-4, 2]/np.max(reduced_data[:-4, 2]), 50, 3),label='Smoothed relative PCA z-coordinate')\n",
    "for p in range((4)):\n",
    "    plt.plot(time,distances[p,:],label=labels[p],c=colors[p])\n",
    "plt.title('1-Relative distance to each cluster over time')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('1 - Relative distance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for p in range((4)):\n",
    "    plt.figure()\n",
    "    df=pd.DataFrame(columns=['distances','smoothed z-coord'])\n",
    "    df['distances']=distances[p,:]\n",
    "    df['z-coord']=reduced_data[:-4, 2]/np.max(reduced_data[:-4, 2])\n",
    "    sns.lmplot(x='distances',y='z-coord', data=df,height=6, aspect=1.5)\n",
    "    plt.text(0.4,0.6,f'R squared = {np.round(scipy.stats.pearsonr(distances[p,:], reduced_data[:-4, 2]/np.max(reduced_data[:-4, 2]))[0],4)}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''plt.figure(figsize=[15,10])\n",
    "plt.scatter(time, cluster_labels, c = [colors[clust] for clust in cluster_labels], label=cluster_labels)\n",
    "plt.yticks(np.arange(0,4,1))\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('State')\n",
    "plt.title('Clustered state according to time')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "distances = np.zeros((4,len(correlation_matrices)))\n",
    "time = np.linspace(1,2400000/1000,len(correlation_matrices))\n",
    "for j in range((4)):\n",
    "    for i in range(len(correlation_matrices)):\n",
    "        distances[j,i] = np.sum(np.abs(correlation_matrices[i]-manual_clusters[j]))\n",
    "\n",
    "sums=np.sum(distances,axis=0)\n",
    "labels=['right','left','top','bottom']\n",
    "\n",
    "for i in range(len(correlation_matrices)):\n",
    "    distances[:,i] = 1-distances[:,i]/sums[i]\n",
    "colors=['red','blue','green','orange','black','purple']\n",
    "\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(time,savgol_filter(reduced_data[:-4, 1]/np.max(reduced_data[:-4, 1]), 50, 3),label='Smoothed relative PCA z-coordinate')\n",
    "for p in range((4)):\n",
    "    plt.plot(time,distances[p,:],label=labels[p],c=colors[p])\n",
    "plt.title('1-Relative distance to each cluster over time')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('1 - Relative distance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for p in range((4)):\n",
    "    plt.figure()\n",
    "    df=pd.DataFrame(columns=['distances','smoothed z-coord'])\n",
    "    df['distances']=distances[p,:]\n",
    "    df['z-coord']=reduced_data[:-4, 1]/np.max(reduced_data[:-4, 1])\n",
    "    sns.lmplot(x='distances',y='z-coord', data=df,height=6, aspect=1.5)\n",
    "    plt.text(0.4,0.6,f'R squared = {np.round(scipy.stats.pearsonr(distances[p,:], reduced_data[:-4, 1]/np.max(reduced_data[:-4, 1]))[0],4)}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''plt.figure(figsize=[15,10])\n",
    "plt.scatter(time, cluster_labels, c = [colors[clust] for clust in cluster_labels], label=cluster_labels)\n",
    "plt.yticks(np.arange(0,4,1))\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('State')\n",
    "plt.title('Clustered state according to time')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "gc_res=[]\n",
    "for p in range((4)):\n",
    "    print(labels[p])\n",
    "    df=pd.DataFrame(columns=['distances','z-coord'])\n",
    "    df['distances']=distances[p,:]\n",
    "    df['z-coord']=reduced_data[:-4, 1]/np.max(reduced_data[:-4, 1])\n",
    "    gc_res.append(grangercausalitytests(df.values, 4,verbose=1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gc_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in manual_clusters : \n",
    "    plt.figure()\n",
    "    plt.imshow(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise level effect investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = [1e-3,1e-5,1e-6] \n",
    "for s in sigma : \n",
    "    data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{s}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 2400000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances = DFC_visu_pipeline(data_corr,meth, window_size=10000,overlap=5000,tmax=t2/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.linspace(1e-6,1e-7,20)#[1e-5,1e-6,9e-7,5e-7,3e-7,2e-7,1.5e-7,1e-7] \n",
    "for s in sigma : \n",
    "    data = np.load(f'{path_save}/testsigmaminsim_dt_0.5_G_{7}_sigma_{s}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 2400000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    fn.build_FCmat(data_corr,'Pearson',num_clusters=3,window_size=2000,overlap=1000,tmax=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## smaller noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.linspace(1e-12,0,20) #sigma = np.linspace(1e-7,1e-12,20)#[1e-5,1e-6,9e-7,5e-7,3e-7,2e-7,1.5e-7,1e-7] \n",
    "for s in sigma : \n",
    "    data = np.load(f'{path_save}/testsigmaminsim_dt_0.5_G_{7}_sigma_{np.round(s,3)}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 2400000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    fn.build_FCmat(data_corr,'Pearson',num_clusters=3,window_size=2000,overlap=1000,tmax=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = [1e-10,1e-12,0] \n",
    "for s in sigma : \n",
    "    data = np.load(f'{path_save}/testsigmaminsim_dt_0.5_G_{7}_sigma_{s}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 2400000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances = DFC_visu_pipeline(data_corr,meth,window_size=2000,overlap=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## z to intrahemispheric relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "sums=np.sum(distances,axis=0)\n",
    "labels=['right','left','top','bottom']\n",
    "\n",
    "for i in range(len(correlation_matrices)):\n",
    "    distances[:,i] = 1-distances[:,i]/sums[i]\n",
    "colors=['red','blue','green','orange','black','purple']\n",
    "\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(time,savgol_filter(reduced_data[:-3, 1]/np.max(reduced_data[:-3, 1]), 50, 3),label='Smoothed relative PCA z-coordinate')\n",
    "for p in range((3)):\n",
    "    plt.plot(time,distances[p,:],label=labels[p],c=colors[p])\n",
    "plt.title('1-Relative distance to each cluster over time')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('1 - Relative distance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for p in range((3)):\n",
    "    plt.figure()\n",
    "    df=pd.DataFrame(columns=['distances','smoothed z-coord'])\n",
    "    df['distances']=distances[p,:]\n",
    "    df['z-coord']=reduced_data[:-3, 1]/np.max(reduced_data[:-3, 1])\n",
    "    sns.lmplot(x='distances',y='z-coord', data=df,height=6, aspect=1.5)\n",
    "    plt.text(0.4,0.6,f'R squared = {np.round(scipy.stats.pearsonr(distances[p,:], reduced_data[:-3, 1]/np.max(reduced_data[:-3, 1]))[0],4)}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''plt.figure(figsize=[15,10])\n",
    "plt.scatter(time, cluster_labels, c = [colors[clust] for clust in cluster_labels], label=cluster_labels)\n",
    "plt.yticks(np.arange(0,4,1))\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('State')\n",
    "plt.title('Clustered state according to time')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study on subjects connectivity : subject TVB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB2.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 300000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca2, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_explained_variance(pca):\n",
    "    return np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_explained_variance(pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G=[7]\n",
    "sigma=[1e-9]\n",
    "#G=[1,1.5,2,3,4,5,6,7,8,9,10,12]\n",
    "for s in range(len(sigma)):\n",
    "    data = np.load(f'{path_save}/testsigmaminsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB10.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 300000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "\n",
    "G=[5,7]\n",
    "sigma = [1e-7,1e-9,1e-11]\n",
    "#G=[1,1.5,2,3,4,5,6,7,8,9,10,12]\n",
    "for g in range(len(G)):\n",
    "    for s in range(len(sigma)):\n",
    "        print(G[g],' ',sigma[s])\n",
    "        data = np.load(f'{path_save}/40sim_dt_0.5_G_{G[g]}_sigma_{sigma[s]}_TVB10.npz')\n",
    "        data_time=data['traw_time']\n",
    "        delta_t = data_time[1]-data_time[0]\n",
    "        sf = 1/(delta_t*1e-3)\n",
    "        win = 1*sf\n",
    "\n",
    "        t1 = 1000\n",
    "        t2 = 2400000 #unit second\n",
    "        time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "        idc1 = time_corr_id[0][0]\n",
    "        idc2= time_corr_id[0][-1]\n",
    "        time_corr=data_time[idc1:idc2]\n",
    "        data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "        print(np.array(data_corr).shape)\n",
    "\n",
    "        methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "        for meth in methods : \n",
    "            closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=10000,overlap=5000,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40sim_dt_0.5_G_{7}_sigma_{1e-9}_TVB10.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca10, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=10000,overlap=5000,tmax=t2/1000, intrahemispheric=True, square=[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animated_plot_cloud(correlation_matrices, reduced_data, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_PCs(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intrahemispheric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(f'{path_save}/40sim_dt_0.5_G_{7}_sigma_{1e-09}_TVB10.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances = DFC_visu_pipeline(data_corr,meth,intrahemispheric=False,square=[0,1], window_size=100, overlap=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA ref transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D PCA\n",
    "num_clusters=3\n",
    "np.random.seed=42\n",
    "ext_data=np.concatenate((correlation_matrices,most_repeating_states))\n",
    "data=np.array(ext_data).reshape(len(correlation_matrices)+num_clusters,len(correlation_matrices[0])**2)\n",
    "#flipped_mtc=[inter_flip(mtx) for mtx in np.array(correlation_matrices)[rmd_idx]]\n",
    "pre_reduced= PCA(n_components=76**2).fit_transform(data)\n",
    "reduced_data = pca_ref.transform(pre_reduced)\n",
    "centroids = reduced_data[-num_clusters:]\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(x=reduced_data[:-num_clusters, 0], y=reduced_data[:-num_clusters, 1], z=reduced_data[:-num_clusters, 2],\n",
    "                                    mode='markers', marker=dict(color=cluster_labels))])\n",
    "\n",
    "centroids = reduced_data[-num_clusters:]\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=centroids[:, 0], y=centroids[:, 1], z=centroids[:, 2],\n",
    "                                mode='markers+text', marker=dict(symbol='x', size=6),\n",
    "                            text=[str(i) for i in range(num_clusters)], name='Centroids', showlegend=False))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=reduced_data[:-num_clusters, 0],y=reduced_data[:-num_clusters, 1],z=reduced_data[:-num_clusters, 2],mode='lines',line=dict(color='black', width=2)))\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Centroids are marked with cross\\n\"\n",
    "    f\"Arrows indicate temporal order\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB5.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 300000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_PCs(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB5.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca5, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=10000,overlap=5000,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animated_plot_cloud(correlation_matrices, reduced_data, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_PCs(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrahemispheric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB5.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca5, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=10000,overlap=5000,tmax=t2/1000, intrahemispheric=True, square=[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animated_plot_cloud(correlation_matrices, reduced_data, cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB7.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 300000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animated_plot_cloud(correlation_matrices, reduced_data, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_PCs(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try normalisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB7.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 300000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "data_corr = preprocessing.normalize(data_corr, axis=0)\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca7, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Initialize the directed functional connectivity matrix\n",
    "connectivity_matrix = np.zeros((84,84))\n",
    "data = data_corr\n",
    "windowed_data = data[1000:1000 + int(2000), :]\n",
    "\n",
    "# Fit a VAR model to the data\n",
    "model = VAR(windowed_data)\n",
    "\n",
    "# Compute the lag order using an information criterion (e.g., AIC or BIC)\n",
    "lag_order = 4\n",
    "\n",
    "# Estimate the coefficients of the VAR model\n",
    "model_fit = model.fit(lag_order)\n",
    "\n",
    "# Compute the Granger causality matrix\n",
    "granger_matrix = model_fit.test_causality(caused=range(84), causing=range(84), kind='f')\n",
    "\n",
    "# Update the directed functional connectivity matrix with the Granger causality values\n",
    "connectivity_matrix = granger_matrix.reshape(84, 84)\n",
    "\n",
    "# Print the directed functional connectivity matrix\n",
    "print(connectivity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(windowed_data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granger_matrix.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "tst=np.zeros(84*84)\n",
    "\n",
    "for i in range(84):\n",
    "    for j in range(84):\n",
    "        df=pd.DataFrame(columns=['1','2'])\n",
    "        df['1']=windowed_data[:,i]\n",
    "        df['2']=windowed_data[:,j]\n",
    "        x=grangercausalitytests(df.values, [2],verbose=1)\n",
    "        tst[i*84+j]=x[2][0]['params_ftest'][1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tst.reshape(84,84))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB8.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 300000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_explained_variance(pca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB9.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 300000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca2, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_explained_variance(pca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB11.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca11, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=10000,overlap=5000,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_explained_variance(pca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB12.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca12, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=10000,overlap=5000,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_explained_variance(pca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCs comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcas=[pca2,pca5,pca7,pca8,pca9,pca10,pca11,pca12]\n",
    "str_pcas=['pca2','pca5','pca7','pca8','pca9','pca10','pca11','pca12']\n",
    "comps=[]\n",
    "for p,pca in enumerate(pcas) : \n",
    "    comps.append(plot_PCs(pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_comps = []\n",
    "for c in comps : \n",
    "    for cc in c : \n",
    "        sp_comps.append(cc)\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-0.05, 0.05))\n",
    "sp_comps = scaler.fit_transform(sp_comps)\n",
    "for i, c in enumerate(sp_comps): \n",
    "    sp_comps = scaler.fit_transform(sp_comps)\n",
    "    for j, com in enumerate(sp_comps): \n",
    "        if i != j: \n",
    "            diff = c - com\n",
    "            plt.figure()\n",
    "            plt.imshow(diff.reshape(84, 84),cmap='bwr')\n",
    "            plt.colorbar();plt.clim([-0.05,0.05])\n",
    "            plt.title(f'PC{np.floor(i/3)+1}, {str_pcas[i%3]} - PC{np.floor(j/3)+1}, {str_pcas[j%3]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Angle evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_angle(point1, point2):\n",
    "    vector1 = np.array(point1)\n",
    "    vector2 = np.array(point2)\n",
    "    \n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    magnitude_product = np.linalg.norm(vector1) * np.linalg.norm(vector2)\n",
    "    \n",
    "    angle = np.arccos(dot_product / magnitude_product)\n",
    "    \n",
    "    return np.degrees(angle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([calculate_angle(reduced_data[i,:],reduced_data[i+1,:]) for i in range(len(reduced_data)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_phase(point1, point2):\n",
    "    phase1 = np.angle(point1,deg=True)\n",
    "    phase2 = np.angle(point2,deg=True)\n",
    "    \n",
    "    phase_diff = phase2 - phase1\n",
    "    \n",
    "    return phase_diff\n",
    "\n",
    "\n",
    "angles = [calculate_angle(reduced_data[i,:],reduced_data[i+1,:]) for i in range(len(reduced_data)-1)]\n",
    "phases = [calculate_phase(reduced_data[i,:],reduced_data[i+1,:]) for i in range(len(reduced_data)-1)]\n",
    "\n",
    "plt.scatter(angles, phases, c=phases, cmap='coolwarm')\n",
    "plt.colorbar(label='Phase Difference')\n",
    "plt.xlabel('Angle')\n",
    "plt.ylabel('Phase Difference')\n",
    "plt.title('Angle vs Phase Difference')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = [calculate_angle(reduced_data[i,:],reduced_data[i+1,:]) for i in range(len(reduced_data)-1)]\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(x=reduced_data[:-num_clusters, 0], y=reduced_data[:-num_clusters, 1], z=reduced_data[:-num_clusters, 2],\n",
    "                                mode='markers', marker=dict(color='black',size=2))])\n",
    "\n",
    "centroids = reduced_data[-num_clusters:]\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=centroids[:, 0], y=centroids[:, 1], z=centroids[:, 2],\n",
    "                                mode='markers+text', marker=dict(symbol='x', size=6),\n",
    "                            text=[str(i) for i in range(num_clusters)], name='Centroids', showlegend=False))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=reduced_data[:-num_clusters, 0],y=reduced_data[:-num_clusters, 1],z=reduced_data[:-num_clusters, 2],mode='lines',line=dict(color=angles*100, width=2)))\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Centroids are marked with cross\\n\"\n",
    "    f\"points are linked according to temporal order\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New plan projection Esra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TVB10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40sim_dt_0.5_G_{7}_sigma_{1e-9}_TVB10.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 300000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca10, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False, square=[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca10.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_PC1 = components[0]\n",
    "new_PC2 = components[1]-components[2]\n",
    "new_PC3 = components[1]+components[2]\n",
    "new_PCs=np.array([new_PC1,new_PC2,new_PC3])\n",
    "\n",
    "for PC in new_PCs : \n",
    "    plt.figure()\n",
    "    plt.imshow(PC.reshape(84,84),cmap='bwr')\n",
    "    plt.colorbar();plt.clim([-0.05,0.05])\n",
    "\n",
    "new_PCs=pca10.transform(new_PCs)\n",
    "new_PC1=new_PCs[0]\n",
    "new_PC2=new_PCs[1]\n",
    "new_PC3=new_PCs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_plane(point1, point2):\n",
    "    # Calculate the normal vector of the plane\n",
    "    normal = np.cross(point1, point2)\n",
    "    return normal\n",
    "\n",
    "# Function to project points onto the plane\n",
    "def project_onto_plane(plane_normal, plane_point, other_points):\n",
    "    # Ensure the plane normal is a unit vector\n",
    "    plane_normal = plane_normal / np.linalg.norm(plane_normal)\n",
    "    \n",
    "    # Calculate the equation of the plane (ax + by + cz = d)\n",
    "    d = np.dot(plane_normal, plane_point)\n",
    "    \n",
    "    # Calculate the projection of other points onto the plane\n",
    "    projections = []\n",
    "    for point in other_points:\n",
    "        t = d - np.dot(plane_normal, point)\n",
    "        projected_point = point + t * plane_normal\n",
    "        projections.append(projected_point)\n",
    "    \n",
    "    return projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan1=define_plane(new_PC1,new_PC2)\n",
    "plan2=define_plane(new_PC1,new_PC3)\n",
    "plan3=define_plane(new_PC2,new_PC3)\n",
    "\n",
    "proj1=np.array(project_onto_plane(plan1, new_PC1, reduced_data))\n",
    "proj2=np.array(project_onto_plane(plan2, new_PC1, reduced_data))\n",
    "proj3=np.array(project_onto_plane(plan3, new_PC2, reduced_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLAN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter3d(x=proj1[:-num_clusters, 0], y=proj1[:-num_clusters, 1], z=proj1[:-num_clusters, 2],\n",
    "                                    mode='markers', marker=dict(color=cluster_labels))])\n",
    "\n",
    "centroids = proj1[-num_clusters:]\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=centroids[:, 0], y=centroids[:, 1], z=centroids[:, 2],\n",
    "                                mode='markers+text', marker=dict(symbol='x', size=6),\n",
    "                            text=[str(i) for i in range(num_clusters)], name='Centroids', showlegend=False))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=proj1[:-num_clusters, 0],y=proj1[:-num_clusters, 1],z=proj1[:-num_clusters, 2],mode='lines',line=dict(color='black', width=2)))\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"Projected PCA; axis1=PC1/ axis2=PC2-PC3\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_3d_to_2d(points_3d, plane_normal):\n",
    "    # Ensure the plane normal is a unit vector\n",
    "    plane_normal = plane_normal / np.linalg.norm(plane_normal)\n",
    "    \n",
    "    # Calculate the projection of points onto the plane\n",
    "    # The projection formula: P = V - dot(V, N) * N, where V is the point and N is the normal vector\n",
    "    projected_points = points_3d - np.dot(points_3d, plane_normal)[:, np.newaxis] * plane_normal\n",
    "    \n",
    "    return projected_points\n",
    "\n",
    "proj1_2d=pd.DataFrame(project_3d_to_2d(proj1,np.cross(proj1[0],proj1[1])))\n",
    "\n",
    "plt.scatter(proj1_2d[0][:-num_clusters],proj1_2d[1][:-num_clusters],c=cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorscale = ['#7A4579', '#D56073', 'rgb(236,158,105)', (1, 1, 0.2), (0.98,0.98,0.98)]\n",
    "\n",
    "fig = ff.create_2d_density(\n",
    "    proj1_2d[0][:-num_clusters],proj1_2d[1][:-num_clusters], colorscale=colorscale,\n",
    "    hist_color='rgb(255, 237, 222)', point_size=3\n",
    ")\n",
    "fig.update_layout(title=f\"Desity plot of projected points\", \n",
    "    height=1000, width=1000)\n",
    "\n",
    "# Add crosses marking the projections of the centroids\n",
    "centroids_proj = project_3d_to_2d(centroids, np.cross(proj1[0], proj1[1]))\n",
    "fig.add_trace(go.Scatter(x=centroids_proj[:, 0], y=centroids_proj[:, 1],\n",
    "                         mode='markers+text', marker=dict(symbol='x', size=20,color='white'),text=[str(i) for i in range(num_clusters)],\n",
    "                         \n",
    "                         name='Centroids'))\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"Desity plot of projected points\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_center=[-0.5,0.5]\n",
    "xs=proj1_2d[:-num_clusters][0]\n",
    "ys=proj1_2d[:-num_clusters][1]\n",
    "\n",
    "dx=manual_center[0]-xs\n",
    "dy=manual_center[1]-ys\n",
    "\n",
    "radial_distance=np.sqrt(dx**2+dy**2)\n",
    "angles=np.arctan2(dy,dx)\n",
    "unwraped=np.unwrap(angles)\n",
    "indeg=np.degrees(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=go.Figure(data=[go.Scatter(x=list(range(len(unwraped))),y=unwraped,mode='markers',marker=dict(color=radial_distance,size=5,showscale=True))])\n",
    "fig.update_layout(coloraxis=dict(colorbar=dict(title='Radial Distance')))\n",
    "fig.update_layout(title=f\"Angle evolution, Subj10, Healthy\", \n",
    "    height=1000, width=1750)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLAN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter3d(x=proj2[:-num_clusters, 0], y=proj2[:-num_clusters, 1], z=proj2[:-num_clusters, 2],\n",
    "                                    mode='markers', marker=dict(color=cluster_labels))])\n",
    "\n",
    "centroids = proj2[-num_clusters:]\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=centroids[:, 0], y=centroids[:, 1], z=centroids[:, 2],\n",
    "                                mode='markers+text', marker=dict(symbol='x', size=6),\n",
    "                            text=[str(i) for i in range(num_clusters)], name='Centroids', showlegend=False))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=proj2[:-num_clusters, 0],y=proj2[:-num_clusters, 1],z=proj2[:-num_clusters, 2],mode='lines',line=dict(color='black', width=2)))\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"Projected PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Centroids are marked with cross\\n\"\n",
    "    f\"points are linked according to temporal order\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj2_2d=pd.DataFrame(project_3d_to_2d(proj2,np.cross(proj2[0],proj2[4])))\n",
    "\n",
    "plt.scatter(proj2_2d[0][:-num_clusters],proj2_2d[1][:-num_clusters],c=cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLAN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter3d(x=proj3[:-num_clusters, 0], y=proj3[:-num_clusters, 1], z=proj3[:-num_clusters, 2],\n",
    "                                    mode='markers', marker=dict(color=cluster_labels))])\n",
    "\n",
    "centroids = proj3[-num_clusters:]\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=centroids[:, 0], y=centroids[:, 1], z=centroids[:, 2],\n",
    "                                mode='markers+text', marker=dict(symbol='x', size=6),\n",
    "                            text=[str(i) for i in range(num_clusters)], name='Centroids', showlegend=False))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=proj3[:-num_clusters, 0],y=proj3[:-num_clusters, 1],z=proj3[:-num_clusters, 2],mode='lines',line=dict(color='black', width=2)))\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"Projected PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Centroids are marked with cross\\n\"\n",
    "    f\"points are linked according to temporal order\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(proj3[:-num_clusters, 2],proj3[:-num_clusters, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorscale = ['#7A4579', '#D56073', 'rgb(236,158,105)', (1, 1, 0.2), (0.98,0.98,0.98)]\n",
    "\n",
    "fig = ff.create_2d_density(\n",
    "    proj3[:-num_clusters, 1], proj3[:-num_clusters, 2], colorscale=colorscale,\n",
    "    hist_color='rgb(255, 237, 222)', point_size=3\n",
    ")\n",
    "plt.scatter()\n",
    "fig.update_layout(title=f\"Projected PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Centroids are marked with cross\\n\"\n",
    "    f\"points are linked according to temporal order\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con1=proj3[:, 1]/proj3[:, 0]\n",
    "con2=proj3[:, 2]/proj3[:, 0]\n",
    "con=np.array([con1,con2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(con[0,:-num_clusters],con[1,:-num_clusters],c=cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/20minsim_dt_0.5_G_{7}_sigma_1e-07.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=10000,overlap=5000,tmax=t2/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_data=np.concatenate((correlation_matrices,most_repeating_states))\n",
    "data=np.array(ext_data).reshape(len(correlation_matrices)+num_clusters,len(correlation_matrices[0])**2)\n",
    "reduced_data_2d = PCA(n_components=2).fit_transform(data)\n",
    "manual_center=[6,0]\n",
    "\n",
    "colorscale = ['#7A4579', '#D56073', 'rgb(236,158,105)', (1, 1, 0.2), (0.98,0.98,0.98)]\n",
    "\n",
    "fig = ff.create_2d_density(\n",
    "    reduced_data_2d[:-num_clusters, 0], reduced_data_2d[:-num_clusters, 1], colorscale=colorscale,\n",
    "    hist_color='rgb(255, 237, 222)', point_size=3\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=[manual_center[0]],y=[manual_center[1]],mode='markers',marker=dict(symbol='x',color='black',size=10)))\n",
    "\n",
    "fig.update_layout(title=f\"Projected PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Centroids are marked with cross\\n\"\n",
    "    f\"points are linked according to temporal order\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angle and radial distance evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_center=[6,0]\n",
    "xs=reduced_data_2d[:-num_clusters, 0]\n",
    "ys=reduced_data_2d[:-num_clusters,1]\n",
    "\n",
    "dx=manual_center[0]-xs\n",
    "dy=manual_center[1]-ys\n",
    "\n",
    "radial_distance=np.sqrt(dx**2+dy**2)\n",
    "angles=np.arctan2(dy,dx)\n",
    "unwraped=np.unwrap(angles)\n",
    "indeg=np.degrees(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(indeg)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('angle')\n",
    "plt.title('Angle of each point with respect to the ellipse center')\n",
    "plt.figure()\n",
    "plt.plot(radial_distance)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('radial distance')\n",
    "plt.title('Radial distance of each point accross time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(unwraped)\n",
    "plt.ylabel('unwraped angle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=go.Figure(data=[go.Scatter(x=list(range(len(unwraped))),y=unwraped,mode='markers',marker=dict(color=radial_distance,size=5,showscale=True))])\n",
    "fig.update_layout(coloraxis=dict(colorbar=dict(title='Radial Distance')))\n",
    "fig.update_layout(title=f\"Angle evolution\", \n",
    "    height=1000, width=1750)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(radial_distance,indeg)\n",
    "plt.title('redial distance vs angle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corr corr Matrix + dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_corr_mat=np.corrcoef(np.array(correlation_matrices).reshape(len(correlation_matrices),len(correlation_matrices[0])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(corr_corr_mat)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_data = linkage(corr_corr_mat, method='ward', metric='euclidean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "k=8\n",
    "dend=dendrogram(linkage_data, orientation='top', color_threshold=8)  \n",
    "clusters=scipy.cluster.hierarchy.fcluster(linkage_data, 8, criterion='maxclust')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "dend=dendrogram(linkage_data, orientation='top', truncate_mode='lastp', p=8, show_leaf_counts=True, show_contracted=True, no_labels=True, color_threshold=100)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=ff.create_dendrogram(corr_corr_mat, color_threshold=4.5)\n",
    "fig.update_layout(width=800, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testi=scipy.cluster.hierarchy.fcluster(linkage_data, 8, criterion='maxclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust1=[]\n",
    "clust2=[]\n",
    "clust3=[]\n",
    "clust4=[]\n",
    "clust5=[]\n",
    "clust6=[]\n",
    "clust7=[]\n",
    "clust8=[]\n",
    "\n",
    "clusts=[ [] for _ in range(8) ]\n",
    "\n",
    "for i in range(len(correlation_matrices)):\n",
    "    clusts[clusters[i]-1].append(correlation_matrices[i])\n",
    "\n",
    "for j in range(len(clusts)):\n",
    "    plt.figure()\n",
    "    plt.imshow(np.mean(clusts[j],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorg_matrices = [clusts[0]+clusts[1]+clusts[2]+clusts[3]+clusts[4]+clusts[5]+clusts[6]+clusts[7]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust1=np.zeros(len(testi[testi==1]))\n",
    "clust2=np.zeros(len(testi[testi==2]))\n",
    "clust3=np.zeros(len(testi[testi==3]))\n",
    "clust4=np.zeros(len(testi[testi==4]))\n",
    "clust5=np.zeros(len(testi[testi==5]))\n",
    "clust6=np.zeros(len(testi[testi==6]))\n",
    "clust7=np.zeros(len(testi[testi==7]))\n",
    "clust8=np.zeros(len(testi[testi==8]))\n",
    "\n",
    "for i in range(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "# get data\n",
    "data = np.genfromtxt(\"http://files.figshare.com/2133304/ExpRawData_E_TABM_84_A_AFFY_44.tab\",\n",
    "                     names=True,usecols=tuple(range(1,30)),dtype=float, delimiter=\"\\t\")\n",
    "data_array = data.view((float, len(data.dtype.names)))\n",
    "data_array = corr_corr_mat\n",
    "\n",
    "\n",
    "# Initialize figure by creating upper dendrogram\n",
    "fig = ff.create_dendrogram(data_array, orientation='bottom')\n",
    "for i in range(len(fig['data'])):\n",
    "    fig['data'][i]['yaxis'] = 'y2'\n",
    "\n",
    "# Create Side Dendrogram\n",
    "dendro_side = ff.create_dendrogram(data_array, orientation='right')\n",
    "for i in range(len(dendro_side['data'])):\n",
    "    dendro_side['data'][i]['xaxis'] = 'x2'\n",
    "\n",
    "# Add Side Dendrogram Data to Figure\n",
    "for data in dendro_side['data']:\n",
    "    fig.add_trace(data)\n",
    "\n",
    "# Create Heatmap\n",
    "dendro_leaves = dendro_side['layout']['yaxis']['ticktext']\n",
    "dendro_leaves = list(map(int, dendro_leaves))\n",
    "data_dist = pdist(data_array)\n",
    "heat_data = squareform(data_dist)\n",
    "heat_data = heat_data[dendro_leaves,:]\n",
    "heat_data = heat_data[:,dendro_leaves]\n",
    "\n",
    "heatmap = [\n",
    "    go.Heatmap(\n",
    "        x = dendro_leaves,\n",
    "        y = dendro_leaves,\n",
    "        z = heat_data,\n",
    "        colorscale = 'Blues'\n",
    "    )\n",
    "]\n",
    "\n",
    "heatmap[0]['x'] = fig['layout']['xaxis']['tickvals']\n",
    "heatmap[0]['y'] = dendro_side['layout']['yaxis']['tickvals']\n",
    "\n",
    "# Add Heatmap Data to Figure\n",
    "for data in heatmap:\n",
    "    fig.add_trace(data)\n",
    "\n",
    "# Edit Layout\n",
    "fig.update_layout({'width':2000, 'height':2000,\n",
    "                         'showlegend':False, 'hovermode': 'closest',\n",
    "                         })\n",
    "# Edit xaxis\n",
    "fig.update_layout(xaxis={'domain': [.15, 1],\n",
    "                                  'mirror': False,\n",
    "                                  'showgrid': False,\n",
    "                                  'showline': False,\n",
    "                                  'zeroline': False,\n",
    "                                  'ticks':\"\"})\n",
    "# Edit xaxis2\n",
    "fig.update_layout(xaxis2={'domain': [0, .15],\n",
    "                                   'mirror': False,\n",
    "                                   'showgrid': False,\n",
    "                                   'showline': False,\n",
    "                                   'zeroline': False,\n",
    "                                   'showticklabels': False,\n",
    "                                   'ticks':\"\"})\n",
    "\n",
    "# Edit yaxis\n",
    "fig.update_layout(yaxis={'domain': [0, .85],\n",
    "                                  'mirror': False,\n",
    "                                  'showgrid': False,\n",
    "                                  'showline': False,\n",
    "                                  'zeroline': False,\n",
    "                                  'showticklabels': False,\n",
    "                                  'ticks': \"\"\n",
    "                        })\n",
    "# Edit yaxis2\n",
    "fig.update_layout(yaxis2={'domain':[.825, .975],\n",
    "                                   'mirror': False,\n",
    "                                   'showgrid': False,\n",
    "                                   'showline': False,\n",
    "                                   'zeroline': False,\n",
    "                                   'showticklabels': False,\n",
    "                                   'ticks':\"\"})\n",
    "\n",
    "# Plot!\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timesteps investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestep 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/20minsim_dt_0.5_G_{7}_sigma_1e-07.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 600000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=10000,overlap=5000,tmax=t2/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestep 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [40,50,60]\n",
    "for s in seeds :\n",
    "    path_conn = 'D:/Fariba/SC'\n",
    "    path_save = f'D:/Fariba/Timestep_investigation'\n",
    "    data = np.load(f'{path_save}/5minsim_dt_0.3_G_{7}_sigma_1e-07_seed{s}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 600000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=int(10000*0.5/0.3),overlap=int(5000*0.5/0.3),tmax=t2/1000,animated_visu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestep 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [40,50,60]\n",
    "for s in seeds :\n",
    "    path_conn = 'D:/Fariba/SC'\n",
    "    path_save = f'D:/Fariba/Timestep_investigation'\n",
    "    data = np.load(f'{path_save}/5minsim_dt_0.2_G_{7}_sigma_1e-07_seed{s}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 600000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=int(10000*0.5/0.2),overlap=int(5000*0.5/0.2),tmax=t2/1000,animated_visu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestep 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [40,50,60]\n",
    "for s in seeds :\n",
    "    path_conn = 'D:/Fariba/SC'\n",
    "    path_save = f'D:/Fariba/Timestep_investigation'\n",
    "    data = np.load(f'{path_save}/5minsim_dt_0.1_G_{7}_sigma_1e-07_seed{s}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 600000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=int(10000*0.5/0.1),overlap=int(5000*0.5/0.1),tmax=t2/1000,animated_visu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestep 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [40,50,60]\n",
    "for s in seeds :\n",
    "    path_conn = 'D:/Fariba/SC'\n",
    "    path_save = f'D:/Fariba/Timestep_investigation'\n",
    "    data = np.load(f'{path_save}/5minsim_dt_0.05_G_{7}_sigma_1e-07_seed{s}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 600000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=int(10000*0.5/0.05),overlap=int(5000*0.5/0.05),tmax=t2/1000,animated_visu=False,ts=0.05)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
