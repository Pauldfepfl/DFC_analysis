{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tvb.simulator.lab as tsl\n",
    "import mne_connectivity \n",
    "import tvb\n",
    "from scipy import signal\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy \n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import pandas as pd\n",
    "import netplotbrain\n",
    "\n",
    "import functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DFC_visu_pipeline(data, cor_method, num_clusters=3, window_size=2000, overlap=1000,tmax=30, intrahemispheric=False, square=[0,0],animated_visu=True,ts=0.5):\n",
    "    '''\n",
    "    This function performs visualisations regarding Dynamical Functional connectivity (DFC). \n",
    "\n",
    "    ------------\n",
    "    INPUTS\n",
    "    ------------\n",
    "\n",
    "    data : array_like, shape = (signal_duration/timestep,num_regions) \n",
    "        The signal data, typically eeg signal or source level signal. \n",
    "    cor_method : string\n",
    "        The correlation method to build the connectivity matrices. Can be chosen over 'Pearson', 'Spearman', 'Coherence', 'Phase-lock' and 'wpli'.\n",
    "    num_clusters : int, default = 3\n",
    "        Number of clusters for the k-means clustering.\n",
    "    window_size : int, default = 2000  \n",
    "        window_duration[ms]/timestep[ms]\n",
    "    overlap : int or float, default = 1000 \n",
    "        Time of overlap [ms] between two succint windows. \n",
    "    tmax : int, default = 30\n",
    "        Time of the simulation, for visualisation purpose. \n",
    "    intrahemispheric : bool, default = False\n",
    "        Visualisation of clustering of a single square of the connectivity matrix. \n",
    "    square : 2x2 list, default = [0,0]\n",
    "        If intrahemispheric = True, specifies the square of the matrix to perform the analysis on. \n",
    "    animated_visu : bool, default = True\n",
    "        Plot and save an animated plot showing the evolution of correlation matrices and pca visualisation. \n",
    "\n",
    "\n",
    "    ------------\n",
    "    OUTPUTS\n",
    "    ------------\n",
    "    closest_mats : array_like, shape = (num_clusters, num_regions,num_regions)\n",
    "        Closest connectivity matrix to every cluster. \n",
    "    most_repeating_states : array_like, shape = (num_clusters, num_regions,num_regions)\n",
    "         Kmeans clusters centroids. \n",
    "    correlation_matrices : array_like, shape = (num_windows, num_regions,num_regions)\n",
    "        Set of correlation matrics, one per window. \n",
    "    cluster_labels : list, len = num_clusters\n",
    "        Cluster labels.\n",
    "    distances : array_like, shape = (num_clusters, num_windows)\n",
    "         Matrix storing the distances between each connectivity matrix and each cluster. \n",
    "    pca : Sklearn PCA object.\n",
    "        PCA object used for the connectivity matrices dimension reduction.   \n",
    "    reduced_data : array_like\n",
    "        3D PCA reduced data.\n",
    "    '''\n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,num_clusters = fn.build_FCmat(data,cor_method,num_clusters,window_size,overlap,tmax,intrahemispheric,square=square)\n",
    "    distances = fn.compute_clst_dist(most_repeating_states, correlation_matrices, cluster_labels,tmax,num_clusters) \n",
    "    pca, reduced_data = fn.plot_PCA(correlation_matrices, most_repeating_states, cluster_labels,num_clusters)\n",
    "    if animated_visu : \n",
    "        fn.animated_plot(correlation_matrices,reduced_data,cluster_labels)\n",
    "\n",
    "    return closest_mats,most_repeating_states, correlation_matrices, cluster_labels, distances, pca, reduced_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects of window len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wins=[0.5,1,1.5,2,3,4,5,8,10] #s\n",
    "timestep = 0.5 #ms\n",
    "Wins=[i/timestep for i in Wins]\n",
    "# Number of clusters (most repeating states)\n",
    "num_clusters = 3\n",
    "mats=np.zeros((len(Wins), num_clusters,76,76))\n",
    "for i,w in enumerate(Wins):\n",
    "    data = np.load(f'{path_save}/test1.5minsim_dt_0.5_G_{7}_sigma_1e-07.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 80000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels, distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth,window_size=w*1000,overlap=w*500,tmax=t2/1000,animated_visu=False)\n",
    "        mats[i]=closest_mats\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 40 Minutes stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/20minsim_dt_0.5_G_{7}_sigma_1e-07.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 1200000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=10000,overlap=5000,tmax=t2/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_colors = ['red', 'blue', 'green']\n",
    "colors=[]\n",
    "sizes=[]\n",
    "for i in range(len(correlation_matrices)):\n",
    "    if i<21:\n",
    "        colors.append([custom_colors[cluster_labels[j]] for j in range(i+1)])\n",
    "        sizes.append((i+1)*[10])\n",
    "    else : \n",
    "        colors.append(['black'] * (i-20) + [custom_colors[cluster_labels[j]] for j in range(i-20,i)])\n",
    "        sizes.append((i-20)*[4]+20*[10])\n",
    "\n",
    "\n",
    "# Create the figure with subplots\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['Matrix Evolution', 'PCA Evolution'],specs=[\n",
    "        [{\"type\": \"Heatmap\"}, {\"type\": \"scatter3d\"}]])\n",
    "\n",
    "# Initialize the first subplot with the initial matrix heatmap\n",
    "heatmap_trace = go.Heatmap(z=np.flip(correlation_matrices[0], axis=0), colorscale='Viridis')\n",
    "fig.add_trace(heatmap_trace, row=1, col=1)\n",
    "\n",
    "# Initialize the second subplot with the initial PCA scatter plot\n",
    "scatter_trace = go.Scatter3d(x=reduced_data[:, 0], y=reduced_data[:, 1], z=reduced_data[:, 2], mode='markers',marker=dict(color=cluster_labels,size=2,colorscale=custom_colors ))\n",
    "fig.add_trace(scatter_trace, row=1, col=2)\n",
    "fig.frames=[go.Frame(data=[go.Heatmap(z=np.flip(correlation_matrices[i], axis=0), colorscale='Viridis'),go.Scatter3d(x=reduced_data[:i, 0], y=reduced_data[:i, 1], z=reduced_data[:i, 2], mode='markers',marker=dict(color=cluster_labels,size=sizes[i],colorscale=custom_colors ))], traces=[0,1]) for i in range(1,len(correlation_matrices))]\n",
    "\n",
    "# Update layout to include animation settings\n",
    "fig.update_layout(updatemenus=[dict(type='buttons',\n",
    "                                showactive=False,\n",
    "                                buttons=[dict(label='Play',\n",
    "                                                method='animate',\n",
    "                                                args=[None, dict(frame=dict(duration=10, redraw=True), fromcurrent=True)])\n",
    "                                                , \n",
    "                                                {\"args\": [[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\n",
    "                                                        \"mode\": \"immediate\",\n",
    "                                                        \"transition\": {\"duration\": 0}}],\n",
    "                                                        \"label\": \"Pause\",\n",
    "                                                        \"method\": \"animate\"\n",
    "                                                }])],height=1000, width=2000)\n",
    "\n",
    "fig.show()\n",
    "pio.write_html(fig,auto_play=False,file='cloud_template_conn.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_colors = ['red', 'blue', 'green']\n",
    "\n",
    "# Create the figure with subplots\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['Matrix Evolution', 'PCA Evolution'],specs=[\n",
    "        [{\"type\": \"Heatmap\"}, {\"type\": \"scatter3d\"}]])\n",
    "\n",
    "# Initialize the first subplot with the initial matrix heatmap\n",
    "heatmap_trace = go.Heatmap(z=np.flip(correlation_matrices[0], axis=0), colorscale='Viridis')\n",
    "#heatmap_frame = [go.Frame(data=[go.Heatmap(z=correlation_matrices[i], colorscale='Viridis')], name=f'Frame{i}') for i in range(1, len(correlation_matrices))]\n",
    "fig.add_trace(heatmap_trace, row=1, col=1)\n",
    "\n",
    "# Initialize the second subplot with the initial PCA scatter plot\n",
    "scatter_trace = go.Scatter3d(x=reduced_data[:, 0], y=reduced_data[:, 1], z=reduced_data[:, 2], mode='markers',marker=dict(color=cluster_labels,size=2,colorscale=custom_colors ))\n",
    "#scatter_frame = [go.Frame(data=[go.Scatter3d(x=reduced_data[:i, 0], y=reduced_data[:i, 1], z=reduced_data[:i, 2], mode='markers')], name=f'Frame{i}') for i in range(1, len(correlation_matrices))]\n",
    "fig.add_trace(scatter_trace, row=1, col=2)\n",
    "fig.frames=[go.Frame(data=[go.Heatmap(z=np.flip(correlation_matrices[i], axis=0), colorscale='Viridis'),go.Scatter3d(x=reduced_data[:i, 0], y=reduced_data[:i, 1], z=reduced_data[:i, 2], mode='markers',marker=dict(color=cluster_labels,size=2,colorscale=custom_colors ))], traces=[0,1]) for i in range(1,len(correlation_matrices))]\n",
    "# Update layout to include animation settings\n",
    "\n",
    "fig.update_layout(updatemenus=[dict(type='buttons',\n",
    "                                    showactive=False,\n",
    "                                    buttons=[dict(label='Play',\n",
    "                                                  method='animate',\n",
    "                                                  args=[None, dict(frame=dict(duration=10, redraw=True), fromcurrent=True)])\n",
    "                                                  , \n",
    "                                                  {\"args\": [[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\n",
    "                                                        \"mode\": \"immediate\",\n",
    "                                                        \"transition\": {\"duration\": 0}}],\n",
    "                                                        \"label\": \"Pause\",\n",
    "                                                        \"method\": \"animate\"\n",
    "                                                }])],height=1000, width=2000)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "pio.write_html(fig,auto_play=False,file='animated_plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances = DFC_visu_pipeline(data_corr,meth, window_size=10000,overlap=5000,tmax=t2/1000, num_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipping hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_flip(mat):\n",
    "    squares=np.zeros((2,2,int(len(mat)/2),int(len(mat)/2)))\n",
    "    for i in range(2) : \n",
    "        for j in range(2):\n",
    "            squares[i,j]=mat[i*int(len(mat)/2):(i+1)*int(len(mat)/2),j*int(len(mat)/2):(j+1)*int(len(mat)/2)]\n",
    "    mat=np.hstack([np.concatenate((squares[1,1],squares[0,1])),np.concatenate((squares[1,0],squares[0,0]))])\n",
    "    return mat\n",
    "\n",
    "def inter_flip(mat):\n",
    "    squares=np.zeros((2,2,int(len(mat)/2),int(len(mat)/2)))\n",
    "    for i in range(2) : \n",
    "        for j in range(2):\n",
    "            squares[i,j]=mat[i*int(len(mat)/2):(i+1)*int(len(mat)/2),j*int(len(mat)/2):(j+1)*int(len(mat)/2)]\n",
    "    mat=np.hstack([np.concatenate((squares[0,0],squares[0,1])),np.concatenate((squares[1,0],squares[1,1]))])\n",
    "    return mat\n",
    "\n",
    "def intra_flip(mat):\n",
    "    squares=np.zeros((2,2,int(len(mat)/2),int(len(mat)/2)))\n",
    "    for i in range(2) : \n",
    "        for j in range(2):\n",
    "            squares[i,j]=mat[i*int(len(mat)/2):(i+1)*int(len(mat)/2),j*int(len(mat)/2):(j+1)*int(len(mat)/2)]\n",
    "    mat=np.hstack([np.concatenate((squares[1,1],squares[1,0])),np.concatenate((squares[0,1],squares[0,0]))])\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D PCA\n",
    "num_clusters=3\n",
    "np.random.seed=42\n",
    "rmd_idx=np.random.randint(1,957,10)\n",
    "ext_data=np.concatenate((correlation_matrices,most_repeating_states))\n",
    "data=np.array(ext_data).reshape(len(correlation_matrices)+num_clusters,5776)\n",
    "#flipped_mtc=[inter_flip(mtx) for mtx in np.array(correlation_matrices)[rmd_idx]]\n",
    "flipped_mtc=[intra_flip(mtx) for mtx in np.array(correlation_matrices)[rmd_idx]]\n",
    "pca_ref=PCA(n_components=3)\n",
    "reduced_data = pca_ref.fit_transform(data)\n",
    "flip_rshp = np.array(flipped_mtc).reshape(len(flipped_mtc),5776)\n",
    "flip_pca=pca_ref.transform(flip_rshp)\n",
    "centroids = reduced_data[-num_clusters:]\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(x=reduced_data[:-3, 0], y=reduced_data[:-3, 1], z=reduced_data[:-3, 2],\n",
    "                                mode='markers', marker=dict(color='black'), showlegend=False)])\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=reduced_data[rmd_idx, 0], y=reduced_data[rmd_idx, 1], z=reduced_data[rmd_idx, 2],\n",
    "                                mode='markers', marker=dict(color='red'), name='reference points'))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=flip_pca[:, 0], y=flip_pca[:, 1], z=flip_pca[:, 2],\n",
    "                                mode='markers', marker=dict(color='orange'), name='intrahemispheric flipped references'))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=centroids[:, 0], y=centroids[:, 1], z=centroids[:, 2],\n",
    "                                mode='markers+text', marker=dict(symbol='x', size=6),\n",
    "                            text=[str(i) for i in range(num_clusters)], name='Centroids', showlegend=False))\n",
    "for i in range(len(rmd_idx)):\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[reduced_data[rmd_idx[i], 0], flip_pca[i, 0]],\n",
    "        y=[reduced_data[rmd_idx[i], 1], flip_pca[i, 1]],\n",
    "        z=[reduced_data[rmd_idx[i], 2], flip_pca[i, 2]],\n",
    "        mode='lines',\n",
    "        line=dict(color='blue'),\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Centroids are marked with cross\\n\"\n",
    "    f\"Arrows indicate temporal order\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(correlation_matrices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(square_flip(correlation_matrices[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.components_\n",
    "for comp in components : \n",
    "    plt.figure()\n",
    "    plt.imshow(comp.reshape(76,76),cmap='bwr')\n",
    "    plt.colorbar();plt.clim([-0.05,0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distance to cluster + z-axis coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "distances = np.zeros((num_clusters,len(correlation_matrices)))\n",
    "time = np.linspace(1,2400000/1000,len(correlation_matrices))\n",
    "for j in range((num_clusters)):\n",
    "    for i in range(len(correlation_matrices)):\n",
    "        distances[j,i] = np.sum(np.abs(correlation_matrices[i]-most_repeating_states[j]))\n",
    "\n",
    "sums=np.sum(distances,axis=0)\n",
    "\n",
    "for i in range(len(correlation_matrices)):\n",
    "    distances[:,i] = 1-distances[:,i]/sums[i]\n",
    "\n",
    "colors=['red','blue','green','orange','black','purple']\n",
    "\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(time,savgol_filter(reduced_data[:-num_clusters, 2]/np.max(reduced_data[:, 2]), 40, 4),label='Smoothed relative PCA z-coordinate')\n",
    "for p in range((num_clusters)):\n",
    "    plt.plot(time,distances[p,:],label=str(p),c=colors[p])\n",
    "plt.title('1-Relative distance to each cluster over time')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('1 - Relative distance')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.scatter(time, cluster_labels, c = [colors[clust] for clust in cluster_labels], label=cluster_labels)\n",
    "plt.yticks(np.arange(0,num_clusters,1))\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('State')\n",
    "plt.title('Clustered state according to time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 min sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/20minsim_dt_0.5_G_{7}_sigma_1e-07.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 600000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=10000,overlap=5000,tmax=t2/1000,animated_visu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual cluster creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mat in most_repeating_states : \n",
    "    plt.figure()\n",
    "    plt.imshow(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D PCA\n",
    "num_clusters=3\n",
    "manual_clusters = [most_repeating_states[0],most_repeating_states[1],pca.inverse_transform([10,-5.5,0]).reshape(76,76),pca.inverse_transform([10,7,0]).reshape(76,76)]\n",
    "ext_data=np.concatenate((correlation_matrices,manual_clusters))\n",
    "data=np.array(ext_data).reshape(len(correlation_matrices)+4,5776)\n",
    "pca=PCA(n_components=3)\n",
    "reduced_data = pca.fit_transform(data)\n",
    "manual_clusters = [most_repeating_states[0],most_repeating_states[1],pca.inverse_transform([10,-5.5,0]).reshape(76,76),pca.inverse_transform([10,7,0]).reshape(76,76)]\n",
    "\n",
    "centroids = reduced_data[-4:,:]\n",
    "labels=['right','left','top','bottom']\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(x=reduced_data[:-4, 0], y=reduced_data[:-4, 1], z=reduced_data[:-4, 2],\n",
    "                                mode='markers', marker=dict(color='black', size=3))])\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=centroids[:, 0], y=centroids[:, 1], z=centroids[:, 2],\n",
    "                                mode='markers+text', marker=dict(symbol='x', size=7),\n",
    "                            text=[labels[i] for i in range(len(labels))], name='Centroids', showlegend=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Manually added Centroids are marked with cross\\n\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pca.inverse_transform([10,-5.5,0]).reshape(76,76))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "distances = np.zeros((4,len(correlation_matrices)))\n",
    "time = np.linspace(1,2400000/1000,len(correlation_matrices))\n",
    "for j in range((4)):\n",
    "    for i in range(len(correlation_matrices)):\n",
    "        distances[j,i] = np.sum(np.abs(correlation_matrices[i]-manual_clusters[j]))\n",
    "\n",
    "sums=np.sum(distances,axis=0)\n",
    "labels=['right','left','top','bottom']\n",
    "\n",
    "for i in range(len(correlation_matrices)):\n",
    "    distances[:,i] = 1-distances[:,i]/sums[i]\n",
    "colors=['red','blue','green','orange','black','purple']\n",
    "\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(time,savgol_filter(reduced_data[:-4, 2]/np.max(reduced_data[:-4, 2]), 50, 3),label='Smoothed relative PCA z-coordinate')\n",
    "for p in range((4)):\n",
    "    plt.plot(time,distances[p,:],label=labels[p],c=colors[p])\n",
    "plt.title('1-Relative distance to each cluster over time')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('1 - Relative distance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for p in range((4)):\n",
    "    plt.figure()\n",
    "    df=pd.DataFrame(columns=['distances','smoothed z-coord'])\n",
    "    df['distances']=distances[p,:]\n",
    "    df['z-coord']=reduced_data[:-4, 2]/np.max(reduced_data[:-4, 2])\n",
    "    sns.lmplot(x='distances',y='z-coord', data=df,height=6, aspect=1.5)\n",
    "    plt.text(0.4,0.6,f'R squared = {np.round(scipy.stats.pearsonr(distances[p,:], reduced_data[:-4, 2]/np.max(reduced_data[:-4, 2]))[0],4)}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''plt.figure(figsize=[15,10])\n",
    "plt.scatter(time, cluster_labels, c = [colors[clust] for clust in cluster_labels], label=cluster_labels)\n",
    "plt.yticks(np.arange(0,4,1))\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('State')\n",
    "plt.title('Clustered state according to time')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "distances = np.zeros((4,len(correlation_matrices)))\n",
    "time = np.linspace(1,2400000/1000,len(correlation_matrices))\n",
    "for j in range((4)):\n",
    "    for i in range(len(correlation_matrices)):\n",
    "        distances[j,i] = np.sum(np.abs(correlation_matrices[i]-manual_clusters[j]))\n",
    "\n",
    "sums=np.sum(distances,axis=0)\n",
    "labels=['right','left','top','bottom']\n",
    "\n",
    "for i in range(len(correlation_matrices)):\n",
    "    distances[:,i] = 1-distances[:,i]/sums[i]\n",
    "colors=['red','blue','green','orange','black','purple']\n",
    "\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(time,savgol_filter(reduced_data[:-4, 1]/np.max(reduced_data[:-4, 1]), 50, 3),label='Smoothed relative PCA z-coordinate')\n",
    "for p in range((4)):\n",
    "    plt.plot(time,distances[p,:],label=labels[p],c=colors[p])\n",
    "plt.title('1-Relative distance to each cluster over time')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('1 - Relative distance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for p in range((4)):\n",
    "    plt.figure()\n",
    "    df=pd.DataFrame(columns=['distances','smoothed z-coord'])\n",
    "    df['distances']=distances[p,:]\n",
    "    df['z-coord']=reduced_data[:-4, 1]/np.max(reduced_data[:-4, 1])\n",
    "    sns.lmplot(x='distances',y='z-coord', data=df,height=6, aspect=1.5)\n",
    "    plt.text(0.4,0.6,f'R squared = {np.round(scipy.stats.pearsonr(distances[p,:], reduced_data[:-4, 1]/np.max(reduced_data[:-4, 1]))[0],4)}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''plt.figure(figsize=[15,10])\n",
    "plt.scatter(time, cluster_labels, c = [colors[clust] for clust in cluster_labels], label=cluster_labels)\n",
    "plt.yticks(np.arange(0,4,1))\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('State')\n",
    "plt.title('Clustered state according to time')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "gc_res=[]\n",
    "for p in range((4)):\n",
    "    print(labels[p])\n",
    "    df=pd.DataFrame(columns=['distances','z-coord'])\n",
    "    df['distances']=distances[p,:]\n",
    "    df['z-coord']=reduced_data[:-4, 1]/np.max(reduced_data[:-4, 1])\n",
    "    gc_res.append(grangercausalitytests(df.values, 4,verbose=1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gc_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in manual_clusters : \n",
    "    plt.figure()\n",
    "    plt.imshow(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigger levels of noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = [1e-3,1e-5,1e-6] \n",
    "for s in sigma : \n",
    "    data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{s}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 2400000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances = DFC_visu_pipeline(data_corr,meth, window_size=10000,overlap=5000,tmax=t2/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.linspace(1e-6,1e-7,20)#[1e-5,1e-6,9e-7,5e-7,3e-7,2e-7,1.5e-7,1e-7] \n",
    "for s in sigma : \n",
    "    data = np.load(f'{path_save}/testsigmaminsim_dt_0.5_G_{7}_sigma_{s}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 2400000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    build_FCmat(data_corr,'Pearson',num_clusters=3,window_size=2000,overlap=1000,tmax=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## smaller noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.linspace(1e-12,0,20) #sigma = np.linspace(1e-7,1e-12,20)#[1e-5,1e-6,9e-7,5e-7,3e-7,2e-7,1.5e-7,1e-7] \n",
    "for s in sigma : \n",
    "    data = np.load(f'{path_save}/testsigmaminsim_dt_0.5_G_{7}_sigma_{np.round(s,3)}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 2400000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    build_FCmat(data_corr,'Pearson',num_clusters=3,window_size=2000,overlap=1000,tmax=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = [1e-10,1e-12,0] \n",
    "for s in sigma : \n",
    "    data = np.load(f'{path_save}/testsigmaminsim_dt_0.5_G_{7}_sigma_{s}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 2400000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances = DFC_visu_pipeline(data_corr,meth,window_size=2000,overlap=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long term intrahemispheric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(f'{path_save}/20minsim_dt_0.5_G_{7}_sigma_1e-07.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=10000,overlap=5000,tmax=t2/1000, intrahemispheric=True, square=[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## z to intrahemispheric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "sums=np.sum(distances,axis=0)\n",
    "labels=['right','left','top','bottom']\n",
    "\n",
    "for i in range(len(correlation_matrices)):\n",
    "    distances[:,i] = 1-distances[:,i]/sums[i]\n",
    "colors=['red','blue','green','orange','black','purple']\n",
    "\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(time,savgol_filter(reduced_data[:-3, 1]/np.max(reduced_data[:-3, 1]), 50, 3),label='Smoothed relative PCA z-coordinate')\n",
    "for p in range((3)):\n",
    "    plt.plot(time,distances[p,:],label=labels[p],c=colors[p])\n",
    "plt.title('1-Relative distance to each cluster over time')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('1 - Relative distance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for p in range((3)):\n",
    "    plt.figure()\n",
    "    df=pd.DataFrame(columns=['distances','smoothed z-coord'])\n",
    "    df['distances']=distances[p,:]\n",
    "    df['z-coord']=reduced_data[:-3, 1]/np.max(reduced_data[:-3, 1])\n",
    "    sns.lmplot(x='distances',y='z-coord', data=df,height=6, aspect=1.5)\n",
    "    plt.text(0.4,0.6,f'R squared = {np.round(scipy.stats.pearsonr(distances[p,:], reduced_data[:-3, 1]/np.max(reduced_data[:-3, 1]))[0],4)}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''plt.figure(figsize=[15,10])\n",
    "plt.scatter(time, cluster_labels, c = [colors[clust] for clust in cluster_labels], label=cluster_labels)\n",
    "plt.yticks(np.arange(0,4,1))\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('State')\n",
    "plt.title('Clustered state according to time')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(f'{path_save}/20minsim_dt_0.5_G_{7}_sigma_1e-07.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 30000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['wpli']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances = DFC_visu_pipeline(data_corr,meth, window_size=2000,overlap=1000,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyIF import te_compute as te\n",
    "data = np.load(f'{path_save}/testsigmaminsim_dt_0.5_G_{7}_sigma_{1e-7}.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "conmat=np.zeros((len(data_corr.T),len(data_corr.T)))\n",
    "for i in range(len(data_corr.T)):\n",
    "    for j in range(len(data_corr.T)):\n",
    "        conmat[i,j] = te.te_compute(data_corr[:,i], data_corr[:,j], k=1, embedding=1, safetyCheck=True, GPU=False)\n",
    "\n",
    "plt.imshow(conmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New subject TVB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB2.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 300000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca2, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_explained_variance(pca):\n",
    "    return np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_explained_variance(pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G=[7]\n",
    "sigma=[1e-9]\n",
    "#G=[1,1.5,2,3,4,5,6,7,8,9,10,12]\n",
    "for s in range(len(sigma)):\n",
    "    data = np.load(f'{path_save}/testsigmaminsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB10.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 300000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "\n",
    "G=[5,7]\n",
    "sigma = [1e-7,1e-9,1e-11]\n",
    "#G=[1,1.5,2,3,4,5,6,7,8,9,10,12]\n",
    "for g in range(len(G)):\n",
    "    for s in range(len(sigma)):\n",
    "        print(G[g],' ',sigma[s])\n",
    "        data = np.load(f'{path_save}/40sim_dt_0.5_G_{G[g]}_sigma_{sigma[s]}_TVB10.npz')\n",
    "        data_time=data['traw_time']\n",
    "        delta_t = data_time[1]-data_time[0]\n",
    "        sf = 1/(delta_t*1e-3)\n",
    "        win = 1*sf\n",
    "\n",
    "        t1 = 1000\n",
    "        t2 = 2400000 #unit second\n",
    "        time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "        idc1 = time_corr_id[0][0]\n",
    "        idc2= time_corr_id[0][-1]\n",
    "        time_corr=data_time[idc1:idc2]\n",
    "        data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "        print(np.array(data_corr).shape)\n",
    "\n",
    "        methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "        for meth in methods : \n",
    "            closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=10000,overlap=5000,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40sim_dt_0.5_G_{7}_sigma_{1e-9}_TVB10.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca10, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=10000,overlap=5000,tmax=t2/1000, intrahemispheric=True, square=[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animated_plot_cloud(correlation_matrices, reduced_data, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_PCs(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intrahemispheric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(f'{path_save}/40sim_dt_0.5_G_{7}_sigma_{1e-09}_TVB10.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances = DFC_visu_pipeline(data_corr,meth,intrahemispheric=False,square=[0,1], window_size=100, overlap=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA ref transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D PCA\n",
    "num_clusters=3\n",
    "np.random.seed=42\n",
    "ext_data=np.concatenate((correlation_matrices,most_repeating_states))\n",
    "data=np.array(ext_data).reshape(len(correlation_matrices)+num_clusters,len(correlation_matrices[0])**2)\n",
    "#flipped_mtc=[inter_flip(mtx) for mtx in np.array(correlation_matrices)[rmd_idx]]\n",
    "pre_reduced= PCA(n_components=76**2).fit_transform(data)\n",
    "reduced_data = pca_ref.transform(pre_reduced)\n",
    "centroids = reduced_data[-num_clusters:]\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(x=reduced_data[:-num_clusters, 0], y=reduced_data[:-num_clusters, 1], z=reduced_data[:-num_clusters, 2],\n",
    "                                    mode='markers', marker=dict(color=cluster_labels))])\n",
    "\n",
    "centroids = reduced_data[-num_clusters:]\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=centroids[:, 0], y=centroids[:, 1], z=centroids[:, 2],\n",
    "                                mode='markers+text', marker=dict(symbol='x', size=6),\n",
    "                            text=[str(i) for i in range(num_clusters)], name='Centroids', showlegend=False))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=reduced_data[:-num_clusters, 0],y=reduced_data[:-num_clusters, 1],z=reduced_data[:-num_clusters, 2],mode='lines',line=dict(color='black', width=2)))\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Centroids are marked with cross\\n\"\n",
    "    f\"Arrows indicate temporal order\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB5.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 300000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_PCs(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB5.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca5, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=10000,overlap=5000,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animated_plot_cloud(correlation_matrices, reduced_data, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_PCs(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrahemispheric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB5.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca5, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=10000,overlap=5000,tmax=t2/1000, intrahemispheric=True, square=[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animated_plot_cloud(correlation_matrices, reduced_data, cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB7.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 300000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animated_plot_cloud(correlation_matrices, reduced_data, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_PCs(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try normalisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB7.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 300000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "data_corr = preprocessing.normalize(data_corr, axis=0)\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca7, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Initialize the directed functional connectivity matrix\n",
    "connectivity_matrix = np.zeros((84,84))\n",
    "data = data_corr\n",
    "windowed_data = data[1000:1000 + int(2000), :]\n",
    "\n",
    "# Fit a VAR model to the data\n",
    "model = VAR(windowed_data)\n",
    "\n",
    "# Compute the lag order using an information criterion (e.g., AIC or BIC)\n",
    "lag_order = 4\n",
    "\n",
    "# Estimate the coefficients of the VAR model\n",
    "model_fit = model.fit(lag_order)\n",
    "\n",
    "# Compute the Granger causality matrix\n",
    "granger_matrix = model_fit.test_causality(caused=range(84), causing=range(84), kind='f')\n",
    "\n",
    "# Update the directed functional connectivity matrix with the Granger causality values\n",
    "connectivity_matrix = granger_matrix.reshape(84, 84)\n",
    "\n",
    "# Print the directed functional connectivity matrix\n",
    "print(connectivity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(windowed_data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granger_matrix.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "tst=np.zeros(84*84)\n",
    "\n",
    "for i in range(84):\n",
    "    for j in range(84):\n",
    "        df=pd.DataFrame(columns=['1','2'])\n",
    "        df['1']=windowed_data[:,i]\n",
    "        df['2']=windowed_data[:,j]\n",
    "        x=grangercausalitytests(df.values, [2],verbose=1)\n",
    "        tst[i*84+j]=x[2][0]['params_ftest'][1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tst.reshape(84,84))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB8.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 300000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_explained_variance(pca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB9.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 300000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca2, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_explained_variance(pca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB11.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca11, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=10000,overlap=5000,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_explained_variance(pca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40minsim_dt_0.5_G_{7}_sigma_{1e-9}_TVB12.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca12, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=10000,overlap=5000,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_explained_variance(pca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCs comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcas=[pca2,pca5,pca7,pca8,pca9,pca10,pca11,pca12]\n",
    "str_pcas=['pca2','pca5','pca7','pca8','pca9','pca10','pca11','pca12']\n",
    "comps=[]\n",
    "for p,pca in enumerate(pcas) : \n",
    "    comps.append(plot_PCs(pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_comps = []\n",
    "for c in comps : \n",
    "    for cc in c : \n",
    "        sp_comps.append(cc)\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-0.05, 0.05))\n",
    "sp_comps = scaler.fit_transform(sp_comps)\n",
    "for i, c in enumerate(sp_comps): \n",
    "    sp_comps = scaler.fit_transform(sp_comps)\n",
    "    for j, com in enumerate(sp_comps): \n",
    "        if i != j: \n",
    "            diff = c - com\n",
    "            plt.figure()\n",
    "            plt.imshow(diff.reshape(84, 84),cmap='bwr')\n",
    "            plt.colorbar();plt.clim([-0.05,0.05])\n",
    "            plt.title(f'PC{np.floor(i/3)+1}, {str_pcas[i%3]} - PC{np.floor(j/3)+1}, {str_pcas[j%3]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Angle evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_angle(point1, point2):\n",
    "    vector1 = np.array(point1)\n",
    "    vector2 = np.array(point2)\n",
    "    \n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    magnitude_product = np.linalg.norm(vector1) * np.linalg.norm(vector2)\n",
    "    \n",
    "    angle = np.arccos(dot_product / magnitude_product)\n",
    "    \n",
    "    return np.degrees(angle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([calculate_angle(reduced_data[i,:],reduced_data[i+1,:]) for i in range(len(reduced_data)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_phase(point1, point2):\n",
    "    phase1 = np.angle(point1,deg=True)\n",
    "    phase2 = np.angle(point2,deg=True)\n",
    "    \n",
    "    phase_diff = phase2 - phase1\n",
    "    \n",
    "    return phase_diff\n",
    "\n",
    "\n",
    "angles = [calculate_angle(reduced_data[i,:],reduced_data[i+1,:]) for i in range(len(reduced_data)-1)]\n",
    "phases = [calculate_phase(reduced_data[i,:],reduced_data[i+1,:]) for i in range(len(reduced_data)-1)]\n",
    "\n",
    "plt.scatter(angles, phases, c=phases, cmap='coolwarm')\n",
    "plt.colorbar(label='Phase Difference')\n",
    "plt.xlabel('Angle')\n",
    "plt.ylabel('Phase Difference')\n",
    "plt.title('Angle vs Phase Difference')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = [calculate_angle(reduced_data[i,:],reduced_data[i+1,:]) for i in range(len(reduced_data)-1)]\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(x=reduced_data[:-num_clusters, 0], y=reduced_data[:-num_clusters, 1], z=reduced_data[:-num_clusters, 2],\n",
    "                                mode='markers', marker=dict(color='black',size=2))])\n",
    "\n",
    "centroids = reduced_data[-num_clusters:]\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=centroids[:, 0], y=centroids[:, 1], z=centroids[:, 2],\n",
    "                                mode='markers+text', marker=dict(symbol='x', size=6),\n",
    "                            text=[str(i) for i in range(num_clusters)], name='Centroids', showlegend=False))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=reduced_data[:-num_clusters, 0],y=reduced_data[:-num_clusters, 1],z=reduced_data[:-num_clusters, 2],mode='lines',line=dict(color=angles*100, width=2)))\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Centroids are marked with cross\\n\"\n",
    "    f\"points are linked according to temporal order\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG study Fariba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before stim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = f'D:/Fariba/15Hz/'\n",
    "for i in [5]:#[2,5,7,8,9,10,11,12]:\n",
    "    subj = 'TVB'+str(i)\n",
    "    path_data_eeg =  f'{path_conn}/{subj}/'\n",
    "    data = np.load(f'{path_data_eeg}/segment11.npz')\n",
    "    savg_time = data['savg_time']\n",
    "    savg_data = data['savg_data']\n",
    "    teeg_time = data['eeg_time']\n",
    "    teeg_data = data['eeg_data']\n",
    "    \n",
    "    # desired ouput \n",
    "    savg_y1y2 = savg_data[:, 1, :, 0]-savg_data[:, 2, :, 0]\n",
    "    teeg_y1y2 = (teeg_data[:, 1, :, 0]-teeg_data[:, 2, :, 0])\n",
    "    \n",
    "    t1=1000\n",
    "    t2=30000\n",
    "\n",
    "    id = np.where((savg_time>=t1)*(savg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    source_data = savg_y1y2[idt1:idt2]\n",
    "    \n",
    "    print(np.array(source_data).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats5,most_repeating_states5, correlation_matrices5, cluster_labels5, distances5, pca5, reduced_data5 = DFC_visu_pipeline(source_data,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, num_clusters, figsize=(15, 5))\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # Position of the colorbar\n",
    "\n",
    "for a in range(len(closest_mats5)):\n",
    "    im = axs[a].imshow(closest_mats5[a], cmap='viridis')\n",
    "    axs[a].axis('off')  # Remove axis labels for cleaner visualization\n",
    "    \n",
    "    # Add lines between lines and columns 34 and 49\n",
    "    axs[a].axhline(y=34, color='white', linewidth=2)\n",
    "    axs[a].axhline(y=49, color='white', linewidth=2)\n",
    "    axs[a].axvline(x=34, color='white', linewidth=2)\n",
    "    axs[a].axvline(x=49, color='white', linewidth=2)\n",
    "\n",
    "    if a == 0:\n",
    "        axs[a].text(-2.8, 25, 'Left Cortical', color='black', ha='center', rotation='vertical')\n",
    "        axs[a].text(-2.8, 50, 'Subcortical', color='black', ha='center', rotation='vertical')\n",
    "        axs[a].text(-2.8, 78, 'Right Cortical', color='black', ha='center', rotation='vertical')\n",
    "\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "fig.suptitle(f'Closest matrix to the cluster centers, Stroke', y=0.92)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = f'D:/Fariba/15Hz/'\n",
    "for i in [7]:#[2,5,7,8,9,10,11,12]:\n",
    "    subj = 'TVB'+str(i)\n",
    "    path_data_eeg =  f'{path_conn}/{subj}/'\n",
    "    data = np.load(f'{path_data_eeg}/segment11.npz')\n",
    "    savg_time = data['savg_time']\n",
    "    savg_data = data['savg_data']\n",
    "    teeg_time = data['eeg_time']\n",
    "    teeg_data = data['eeg_data']\n",
    "    \n",
    "    # desired ouput \n",
    "    savg_y1y2 = savg_data[:, 1, :, 0]-savg_data[:, 2, :, 0]\n",
    "    teeg_y1y2 = (teeg_data[:, 1, :, 0]-teeg_data[:, 2, :, 0])\n",
    "    \n",
    "    t1=1000\n",
    "    t2=30000\n",
    "\n",
    "    id = np.where((savg_time>=t1)*(savg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    source_data = savg_y1y2[idt1:idt2]\n",
    "    \n",
    "    print(np.array(source_data).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats7,most_repeating_states7, correlation_matrices7, cluster_labels7,distances7, pca7, reduced_data7 = DFC_visu_pipeline(source_data,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = f'D:/Fariba/15Hz/'\n",
    "for i in [10]:#[2,5,7,8,9,10,11,12]:\n",
    "    subj = 'TVB'+str(i)\n",
    "    path_data_eeg =  f'{path_conn}/{subj}/'\n",
    "    data = np.load(f'{path_data_eeg}/segment11.npz')\n",
    "    savg_time = data['savg_time']\n",
    "    savg_data = data['savg_data']\n",
    "    teeg_time = data['eeg_time']\n",
    "    teeg_data = data['eeg_data']\n",
    "    \n",
    "    # desired ouput \n",
    "    savg_y1y2 = savg_data[:, 1, :, 0]-savg_data[:, 2, :, 0]\n",
    "    teeg_y1y2 = (teeg_data[:, 1, :, 0]-teeg_data[:, 2, :, 0])\n",
    "    \n",
    "    t1=1000\n",
    "    t2=30000\n",
    "\n",
    "    id = np.where((savg_time>=t1)*(savg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    source_data = savg_y1y2[idt1:idt2]\n",
    "    \n",
    "    print(np.array(source_data).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats10,most_repeating_states10, correlation_matrices10, cluster_labels10,distances10, pca10, reduced_data10 = DFC_visu_pipeline(source_data,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matrices figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, num_clusters, figsize=(15, 10))\n",
    "\n",
    "# Plot closest_mats5\n",
    "for a in range(len(closest_mats5)):\n",
    "    im = axs[0, a].imshow(closest_mats5[a], cmap='viridis')\n",
    "    axs[0, a].axis('off')  # Remove axis labels for cleaner visualization\n",
    "    \n",
    "    # Add lines between lines and columns 34 and 49\n",
    "    axs[0, a].axhline(y=34, color='white', linewidth=2)\n",
    "    axs[0, a].axhline(y=49, color='white', linewidth=2)\n",
    "    axs[0, a].axvline(x=34, color='white', linewidth=2)\n",
    "    axs[0, a].axvline(x=49, color='white', linewidth=2)\n",
    "\n",
    "    \n",
    "    if a == 0:\n",
    "        axs[0, a].text(-2.8, 25, 'Left Cortical', color='black', ha='center', rotation='vertical',fontsize=11)\n",
    "        axs[0, a].text(-2.8, 50, 'Subcortical', color='black', ha='center', rotation='vertical',fontsize=11)\n",
    "        axs[0, a].text(-2.8, 78, 'Right Cortical', color='black', ha='center', rotation='vertical',fontsize=11)\n",
    "axs[0, 1].set_title('Stroke',fontsize=22)  # Add title for axs[0]\n",
    "\n",
    "# Plot closest_mats7\n",
    "for a in range(len(closest_mats7)):\n",
    "    im = axs[1, a].imshow(closest_mats7[a], cmap='viridis')\n",
    "    axs[1, a].axis('off')  # Remove axis labels for cleaner visualization\n",
    "    \n",
    "    # Add lines between lines and columns 34 and 49\n",
    "    axs[1, a].axhline(y=34, color='white', linewidth=2)\n",
    "    axs[1, a].axhline(y=49, color='white', linewidth=2)\n",
    "    axs[1, a].axvline(x=34, color='white', linewidth=2)\n",
    "    axs[1, a].axvline(x=49, color='white', linewidth=2)\n",
    "\n",
    "    if a == 0:\n",
    "        axs[1, a].text(-2.8, 25, 'Left Cortical', color='black', ha='center', rotation='vertical',fontsize=11)\n",
    "        axs[1, a].text(-2.8, 50, 'Subcortical', color='black', ha='center', rotation='vertical',fontsize=11)\n",
    "        axs[1, a].text(-2.8, 78, 'Right Cortical', color='black', ha='center', rotation='vertical',fontsize=11)\n",
    "axs[1, 1].set_title('Healthy', fontsize=22)  # Add title for axs[1]\n",
    "\n",
    "# Place colorbar at the bottom horizontally\n",
    "cbar_ax = fig.add_axes([0.15, 0.05, 0.7, 0.02])\n",
    "\n",
    "fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ext_data=np.concatenate((correlation_matrices5,most_repeating_states5))\n",
    "data=np.array(ext_data).reshape(len(correlation_matrices5)+num_clusters,len(correlation_matrices5[0])**2)\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "\n",
    "\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "\n",
    "\n",
    "\n",
    "u=np.diff(reduced_data[:-num_clusters, 0])\n",
    "v=np.diff(reduced_data[:-num_clusters, 1])\n",
    "pos_x = reduced_data[:-num_clusters-1, 0] + u/2\n",
    "pos_y = reduced_data[:-num_clusters-1, 1] + v/2\n",
    "norm = np.sqrt(u**2+v**2) \n",
    "\n",
    "\n",
    "axs[0].plot(reduced_data[:-num_clusters, 0], reduced_data[:-num_clusters, 1], linewidth=0.5)\n",
    "sns.scatterplot(x=reduced_data[:-num_clusters, 0], y=reduced_data[:-num_clusters, 1], size=10, hue=cluster_labels5, palette='Set1', ax=axs[0])\n",
    "#axs[0].legend(loc='upper left')\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "centroids = reduced_data[-num_clusters:]\n",
    "sns.scatterplot(\n",
    "    x=centroids[:, 0],\n",
    "    y=centroids[:, 1],\n",
    "    marker=\"x\",\n",
    "    s=600,\n",
    "    hue=np.linspace(0, num_clusters-1, num_clusters),\n",
    "    palette='Set1',\n",
    "    legend=False,\n",
    "    ax=axs[0]\n",
    ")\n",
    "\n",
    "axs[0].quiver(pos_x, pos_y, u/norm, v/norm, angles=\"xy\", zorder=5, pivot=\"mid\", width=0.001, headwidth=20)\n",
    "\n",
    "axs[0].set_title(\n",
    "    \"Stroke\", fontsize=22\n",
    ")\n",
    "axs[0].set_xlim(x_min-7, x_max)\n",
    "axs[0].set_xticks(())\n",
    "axs[0].set_yticks(())\n",
    "\n",
    "ext_data=np.concatenate((correlation_matrices7,most_repeating_states7))\n",
    "data=np.array(ext_data).reshape(len(correlation_matrices7)+num_clusters,len(correlation_matrices7[0])**2)\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "\n",
    "\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "\n",
    "\n",
    "\n",
    "u=np.diff(reduced_data[:-num_clusters, 0])\n",
    "v=np.diff(reduced_data[:-num_clusters, 1])\n",
    "pos_x = reduced_data[:-num_clusters-1, 0] + u/2\n",
    "pos_y = reduced_data[:-num_clusters-1, 1] + v/2\n",
    "norm = np.sqrt(u**2+v**2) \n",
    "\n",
    "\n",
    "axs[1].plot(reduced_data[:-num_clusters, 0], reduced_data[:-num_clusters, 1], linewidth=0.5)\n",
    "sns.scatterplot(x=reduced_data[:-num_clusters, 0], y=reduced_data[:-num_clusters, 1], size=10, hue=cluster_labels7, palette='Set1', ax=axs[1])\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "centroids = reduced_data[-num_clusters:]\n",
    "sns.scatterplot(\n",
    "    x=centroids[:, 0],\n",
    "    y=centroids[:, 1],\n",
    "    marker=\"x\",\n",
    "    s=600,\n",
    "    hue=np.linspace(0, num_clusters-1, num_clusters),\n",
    "    palette='Set1',\n",
    "    legend=False,\n",
    "    ax=axs[1]\n",
    ")\n",
    "\n",
    "axs[1].quiver(pos_x, pos_y, u/norm, v/norm, angles=\"xy\", zorder=5, pivot=\"mid\", width=0.001, headwidth=20)\n",
    "\n",
    "axs[1].set_title(\n",
    "    \"Healthy\", fontsize=22\n",
    ")\n",
    "axs[1].set_xlim(x_min-7, x_max)\n",
    "axs[1].set_xticks(())\n",
    "axs[1].set_yticks(())\n",
    "axs[1].get_legend().remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximity plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(20, 15))\n",
    "time = np.linspace(1,30,len(correlation_matrices5))\n",
    "\n",
    "colors=['red','blue','green','orange','black','purple']\n",
    "\n",
    "\n",
    "for a in range(num_clusters):    \n",
    "    axs[0].plot(time, distances5[a, :], label=f'State {str(a)}', c=colors[a])\n",
    "    axs[1].plot(time, distances7[a, :], label=f'State {str(a)}', c=colors[a])\n",
    "\n",
    "\n",
    "axs[0].set_title('Stroke', fontsize=30)\n",
    "axs[0].set_ylabel('1-distance',  fontsize=24)\n",
    "axs[0].legend(fontsize=20)\n",
    "axs[0].tick_params(axis='y', labelsize=18)\n",
    "axs[0].tick_params(axis='x', labelsize=0)\n",
    "axs[0].set_ylim(0.6,0.75)\n",
    "\n",
    "axs[1].set_title('Healthy', fontsize=30)\n",
    "axs[1].set_xlabel('Time [s]', fontsize=22)\n",
    "axs[1].set_ylabel('1-distance',  fontsize=22)\n",
    "axs[1].set_ylim(0.6,0.75)\n",
    "\n",
    "axs[1].tick_params(axis='x', labelsize=18)\n",
    "axs[1].tick_params(axis='y', labelsize=18)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, num_clusters, figsize=(25, 15))\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # Position of the colorbar\n",
    "time = np.linspace(1,30,len(correlation_matrices))\n",
    "colors=['red','blue','green','orange','black','purple']\n",
    "\n",
    "# Plot closest_mats5\n",
    "for a in range(len(closest_mats5)):\n",
    "    im = axs[0, a].imshow(closest_mats5[a], cmap='plasma')\n",
    "    axs[0, a].axis('off')  # Remove axis labels for cleaner visualization\n",
    "    \n",
    "    # Add lines between lines and columns 34 and 49\n",
    "    axs[0, a].axhline(y=34, color='white', linewidth=2)\n",
    "    axs[0, a].axhline(y=49, color='white', linewidth=2)\n",
    "    axs[0, a].axvline(x=34, color='white', linewidth=2)\n",
    "    axs[0, a].axvline(x=49, color='white', linewidth=2)\n",
    "\n",
    "    if a == 0:\n",
    "        axs[0, a].text(-2.8, 25, 'Left Cortical', color='black', ha='center', rotation='vertical')\n",
    "        axs[0, a].text(-2.8, 50, 'Subcortical', color='black', ha='center', rotation='vertical')\n",
    "        axs[0, a].text(-2.8, 78, 'Right Cortical', color='black', ha='center', rotation='vertical')\n",
    "axs[0, 1].set_title('Stroke')  # Add title for axs[0]\n",
    "\n",
    "# Plot closest_mats7\n",
    "for a in range(len(closest_mats7)):\n",
    "    im = axs[2, a].imshow(closest_mats7[a], cmap='plasma')\n",
    "    axs[2, a].axis('off')  # Remove axis labels for cleaner visualization\n",
    "    \n",
    "    # Add lines between lines and columns 34 and 49\n",
    "    axs[2, a].axhline(y=34, color='white', linewidth=2)\n",
    "    axs[2, a].axhline(y=49, color='white', linewidth=2)\n",
    "    axs[2, a].axvline(x=34, color='white', linewidth=2)\n",
    "    axs[2, a].axvline(x=49, color='white', linewidth=2)\n",
    "\n",
    "    if a == 0:\n",
    "        axs[2, a].text(-2.8, 25, 'Left Cortical', color='black', ha='center', rotation='vertical')\n",
    "        axs[2, a].text(-2.8, 50, 'Subcortical', color='black', ha='center', rotation='vertical')\n",
    "        axs[2, a].text(-2.8, 78, 'Right Cortical', color='black', ha='center', rotation='vertical')\n",
    "axs[2, 1].set_title('Healthy')  # Add title for axs[1]\n",
    "\n",
    "for a in range(num_clusters):\n",
    "    axs[1, 1].plot(time, distances5[a, :], label=str(a), c=colors[a])\n",
    "    axs[3, 1].plot(time, distances7[a, :], label=str(a), c=colors[a])\n",
    "\n",
    "# Set the limits of the y-axis for the distance plots\n",
    "axs[1, 1].set_ylim([0, 1])\n",
    "axs[3, 1].set_ylim([0, 1])\n",
    "\n",
    "# Remove unnecessary subplots\n",
    "axs[1, 0].remove()\n",
    "axs[3, 0].remove()\n",
    "axs[1, 2].remove()\n",
    "axs[3, 2].remove()\n",
    "\n",
    "# Set common x and y labels for the remaining subplots\n",
    "axs[1, 1].set_xlabel('Time [s]')\n",
    "axs[1, 1].set_ylabel('1 - Relative distance')\n",
    "axs[3, 1].set_xlabel('Time [s]')\n",
    "axs[3, 1].set_ylabel('1 - Relative distance')\n",
    "\n",
    "# Add legends to the remaining subplots\n",
    "axs[1, 1].legend()\n",
    "axs[3, 1].legend()\n",
    "\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "fig.suptitle(f'Comparison of network states without stimulation between stroke and healthy patient', y=0.92)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## During stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = f'D:/Fariba/15Hz/'\n",
    "for i in [5,10]:#[2,5,7,8,9,10,11,12]    \n",
    "    subj = 'TVB'+str(i)\n",
    "    path_data_eeg =  f'{path_conn}/{subj}/'\n",
    "    data = np.load(f'{path_data_eeg}/segment11.npz')\n",
    "    savg_time = data['savg_time']\n",
    "    savg_data = data['savg_data']\n",
    "    teeg_time = data['eeg_time']\n",
    "    teeg_data = data['eeg_data']\n",
    "    \n",
    "    # desired ouput \n",
    "    savg_y1y2 = savg_data[:, 1, :, 0]-savg_data[:, 2, :, 0]\n",
    "    teeg_y1y2 = (teeg_data[:, 1, :, 0]-teeg_data[:, 2, :, 0])\n",
    "    \n",
    "    t1=30000\n",
    "    t2=60000\n",
    "\n",
    "    id = np.where((savg_time>=t1)*(savg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    source_data = savg_y1y2[idt1:idt2]\n",
    "    \n",
    "    print(np.array(source_data).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca12, reduced_data = DFC_visu_pipeline(source_data,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = f'D:/Fariba/15Hz/'\n",
    "for i in [7]:#[2,5,7,8,9,10,11,12]    \n",
    "    subj = 'TVB'+str(i)\n",
    "    path_data_eeg =  f'{path_conn}/{subj}/'\n",
    "    data = np.load(f'{path_data_eeg}/segment11.npz')\n",
    "    savg_time = data['savg_time']\n",
    "    savg_data = data['savg_data']\n",
    "    teeg_time = data['eeg_time']\n",
    "    teeg_data = data['eeg_data']\n",
    "    \n",
    "    # desired ouput \n",
    "    savg_y1y2 = savg_data[:, 1, :, 0]-savg_data[:, 2, :, 0]\n",
    "    teeg_y1y2 = (teeg_data[:, 1, :, 0]-teeg_data[:, 2, :, 0])\n",
    "    \n",
    "    t1=30000\n",
    "    t2=60000\n",
    "\n",
    "    id = np.where((savg_time>=t1)*(savg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    source_data = savg_y1y2[idt1:idt2]\n",
    "    \n",
    "    print(np.array(source_data).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats_stim,most_repeating_states_stim, correlation_matrices_stim, cluster_labels_stim,distances_stim, pca12_stim, reduced_data_stim = DFC_visu_pipeline(source_data,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, num_clusters, figsize=(15, 10))\n",
    "\n",
    "# Plot closest_mats5\n",
    "for a in range(len(closest_mats7)):\n",
    "    im = axs[0, a].imshow(closest_mats7[a], cmap='viridis')\n",
    "    axs[0, a].axis('off')  # Remove axis labels for cleaner visualization\n",
    "    \n",
    "    # Add lines between lines and columns 34 and 49\n",
    "    axs[0, a].axhline(y=34, color='white', linewidth=2)\n",
    "    axs[0, a].axhline(y=49, color='white', linewidth=2)\n",
    "    axs[0, a].axvline(x=34, color='white', linewidth=2)\n",
    "    axs[0, a].axvline(x=49, color='white', linewidth=2)\n",
    "    \n",
    "    # Outline the 13th row and 55th column\n",
    "    axs[0, a].axhline(y=12, color='red', linewidth=0.5)\n",
    "    axs[0, a].axvline(x=54, color='red', linewidth=0.5)\n",
    "    axs[0, a].axhline(y=14, color='red', linewidth=0.5)\n",
    "    axs[0, a].axvline(x=56, color='red', linewidth=0.5)\n",
    "\n",
    "    if a == 0:\n",
    "        axs[0, a].text(-2.8, 25, 'Left Cortical', color='black', ha='center', rotation='vertical',fontsize=11)\n",
    "        axs[0, a].text(-2.8, 50, 'Subcortical', color='black', ha='center', rotation='vertical',fontsize=11)\n",
    "        axs[0, a].text(-2.8, 78, 'Right Cortical', color='black', ha='center', rotation='vertical',fontsize=11)\n",
    "axs[0, 1].set_title('Before stimulation',fontsize=14)  # Add title for axs[0]\n",
    "\n",
    "# Plot closest_mats7\n",
    "for a in range(len(closest_mats_stim)):\n",
    "    im = axs[1, a].imshow(closest_mats_stim[a], cmap='viridis')\n",
    "    axs[1, a].axis('off')  # Remove axis labels for cleaner visualization\n",
    "    \n",
    "    # Add lines between lines and columns 34 and 49\n",
    "    axs[1, a].axhline(y=34, color='white', linewidth=2)\n",
    "    axs[1, a].axhline(y=49, color='white', linewidth=2)\n",
    "    axs[1, a].axvline(x=34, color='white', linewidth=2)\n",
    "    axs[1, a].axvline(x=49, color='white', linewidth=2)\n",
    "    \n",
    "    # Outline the 13th row and 54th column\n",
    "    axs[1, a].axhline(y=12, color='red', linewidth=0.5)\n",
    "    axs[1, a].axvline(x=54, color='red', linewidth=0.5)\n",
    "    axs[1, a].axhline(y=14, color='red', linewidth=0.5)\n",
    "    axs[1, a].axvline(x=56, color='red', linewidth=0.5)\n",
    "\n",
    "    if a == 0:\n",
    "        axs[1, a].text(-2.8, 25, 'Left Cortical', color='black', ha='center', rotation='vertical',fontsize=11)\n",
    "        axs[1, a].text(-2.8, 50, 'Subcortical', color='black', ha='center', rotation='vertical',fontsize=11)\n",
    "        axs[1, a].text(-2.8, 78, 'Right Cortical', color='black', ha='center', rotation='vertical',fontsize=11)\n",
    "axs[1, 1].set_title('During stimulation', fontsize=14)  # Add title for axs[1]\n",
    "\n",
    "# Place colorbar at the bottom horizontally\n",
    "cbar_ax = fig.add_axes([0.15, 0.05, 0.7, 0.02])\n",
    "\n",
    "fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ext_data=np.concatenate((correlation_matrices_stim,most_repeating_states_stim))\n",
    "data=np.array(ext_data).reshape(len(correlation_matrices_stim)+num_clusters,len(correlation_matrices_stim[0])**2)\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "\n",
    "\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "\n",
    "\n",
    "\n",
    "u=np.diff(reduced_data[:-num_clusters, 0])\n",
    "v=np.diff(reduced_data[:-num_clusters, 1])\n",
    "pos_x = reduced_data[:-num_clusters-1, 0] + u/2\n",
    "pos_y = reduced_data[:-num_clusters-1, 1] + v/2\n",
    "norm = np.sqrt(u**2+v**2) \n",
    "\n",
    "\n",
    "axs[0].plot(reduced_data[:-num_clusters, 0], reduced_data[:-num_clusters, 1], linewidth=0.5)\n",
    "sns.scatterplot(x=reduced_data[:-num_clusters, 0], y=reduced_data[:-num_clusters, 1], size=10, hue=cluster_labels_stim, palette='Set1', ax=axs[0])\n",
    "#axs[0].legend(loc='upper left')\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "centroids = reduced_data[-num_clusters:]\n",
    "sns.scatterplot(\n",
    "    x=centroids[:, 0],\n",
    "    y=centroids[:, 1],\n",
    "    marker=\"x\",\n",
    "    s=600,\n",
    "    hue=np.linspace(0, num_clusters-1, num_clusters),\n",
    "    palette='Set1',\n",
    "    legend=False,\n",
    "    ax=axs[0]\n",
    ")\n",
    "\n",
    "axs[0].quiver(pos_x, pos_y, u/norm, v/norm, angles=\"xy\", zorder=5, pivot=\"mid\", width=0.001, headwidth=20)\n",
    "\n",
    "axs[0].set_title(\n",
    "    \"Stroke\", fontsize=22\n",
    ")\n",
    "axs[0].set_xlim(x_min-7, x_max)\n",
    "axs[0].set_xticks(())\n",
    "axs[0].set_yticks(())\n",
    "\n",
    "ext_data=np.concatenate((correlation_matrices7,most_repeating_states7))\n",
    "data=np.array(ext_data).reshape(len(correlation_matrices7)+num_clusters,len(correlation_matrices7[0])**2)\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "\n",
    "\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "\n",
    "\n",
    "\n",
    "u=np.diff(reduced_data[:-num_clusters, 0])\n",
    "v=np.diff(reduced_data[:-num_clusters, 1])\n",
    "pos_x = reduced_data[:-num_clusters-1, 0] + u/2\n",
    "pos_y = reduced_data[:-num_clusters-1, 1] + v/2\n",
    "norm = np.sqrt(u**2+v**2) \n",
    "\n",
    "\n",
    "axs[1].plot(reduced_data[:-num_clusters, 0], reduced_data[:-num_clusters, 1], linewidth=0.5)\n",
    "sns.scatterplot(x=reduced_data[:-num_clusters, 0], y=reduced_data[:-num_clusters, 1], size=10, hue=cluster_labels7, palette='Set1', ax=axs[1])\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "centroids = reduced_data[-num_clusters:]\n",
    "sns.scatterplot(\n",
    "    x=centroids[:, 0],\n",
    "    y=centroids[:, 1],\n",
    "    marker=\"x\",\n",
    "    s=600,\n",
    "    hue=np.linspace(0, num_clusters-1, num_clusters),\n",
    "    palette='Set1',\n",
    "    legend=False,\n",
    "    ax=axs[1]\n",
    ")\n",
    "\n",
    "axs[1].quiver(pos_x, pos_y, u/norm, v/norm, angles=\"xy\", zorder=5, pivot=\"mid\", width=0.001, headwidth=20)\n",
    "\n",
    "axs[1].set_title(\n",
    "    \"Healthy\", fontsize=22\n",
    ")\n",
    "axs[1].set_xlim(x_min-7, x_max)\n",
    "axs[1].set_xticks(())\n",
    "axs[1].set_yticks(())\n",
    "axs[1].get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(25, 20))\n",
    "time = np.linspace(1,30,len(correlation_matrices))\n",
    "\n",
    "colors=['red','blue','green','orange','black','purple']\n",
    "\n",
    "\n",
    "for a in range(num_clusters):    \n",
    "    axs[0].plot(time, distances7[a, :], label=f'State {str(a)}', c=colors[a])\n",
    "    axs[1].plot(time, distances_stim[a, 4:], label=f'State {str(a)}', c=colors[a])\n",
    "\n",
    "\n",
    "axs[0].set_title('Before stimulation', fontsize=30)\n",
    "axs[0].set_xlabel('Time [s]', fontsize=24)\n",
    "axs[0].set_ylabel('1-Distance',  fontsize=24)\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title('After stimulation', fontsize=30)\n",
    "axs[1].set_xlabel('Time [s]', fontsize=24)\n",
    "axs[1].set_ylabel('1-Distance',  fontsize=24)\n",
    "axs[1].legend()\n",
    "\n",
    "fig.suptitle(f'Proximity to each state over time', y=0.94,fontsize=35)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "G=[7]\n",
    "#G=[1,1.5,2,3,4,5,6,7,8,9,10,12]\n",
    "for g in range(len(G)):\n",
    "    path_conn = 'D:/Fariba/SC'\n",
    "    path_save = f'D:/Timing_optimisation'\n",
    "    data = np.load(f'{path_save}/20minsim_dt_0.5_G_{7}_sigma_1e-07.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 2400000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats_temp,most_repeating_states_temp, correlation_matrices_temp, cluster_labels_temp,distances_temp, pca12_temp, reduced_data_temp = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=10000,overlap=5000,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca12_temp.components_\n",
    "    for comp in components : \n",
    "        plt.figure()\n",
    "        plt.imshow(comp.reshape(int(np.sqrt(len(comp))),int(np.sqrt(len(comp)))),cmap='bwr')\n",
    "        plt.colorbar();plt.clim([-0.05,0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_PCs(pca12_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, num_clusters, figsize=(15, 10))\n",
    "\n",
    "# Plot closest_mats5\n",
    "for a in range(len(closest_mats_temp)):\n",
    "    im = axs[ a].imshow(closest_mats_temp[a], cmap='viridis')\n",
    "    axs[ a].axis('off')  # Remove axis labels for cleaner visualization\n",
    "    \n",
    "    # Add lines between lines and columns 34 and 49\n",
    "    axs[ a].axhline(y=37, color='white', linewidth=2)\n",
    "    axs[ a].axvline(x=37, color='white', linewidth=2)\n",
    "\n",
    "    \n",
    "    if a == 0:\n",
    "        axs[ a].text(-2.8, 33, 'Right hemisphere', color='black', ha='center', rotation='vertical',fontsize=11)\n",
    "        axs[ a].text(-2.8, 70, 'Left hemisphere', color='black', ha='center', rotation='vertical',fontsize=11)\n",
    "        axs[ a].text(18, 80, 'Right hemisphere', color='black', ha='center', rotation='horizontal',fontsize=11)\n",
    "        axs[ a].text(55, 80, 'Left hemisphere', color='black', ha='center', rotation='horizontal',fontsize=11)\n",
    "\n",
    "# Place colorbar on the right side vertically\n",
    "cbar_ax = fig.add_axes([0.92, 0.325, 0.02, 0.34])\n",
    "\n",
    "fig.colorbar(im, cax=cbar_ax, orientation='vertical')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_data=np.concatenate((correlation_matrices_temp,most_repeating_states_temp))\n",
    "data=np.array(ext_data).reshape(len(correlation_matrices_temp)+num_clusters,len(correlation_matrices_temp[0])**2)\n",
    "pca_2d_temp=PCA(n_components=2)\n",
    "reduced_data = pca_2d_temp.fit_transform(data)\n",
    "\n",
    "\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "\n",
    "\n",
    "\n",
    "u=np.diff(reduced_data[:-num_clusters, 0])\n",
    "v=np.diff(reduced_data[:-num_clusters, 1])\n",
    "pos_x = reduced_data[:-num_clusters-1, 0] + u/2\n",
    "pos_y = reduced_data[:-num_clusters-1, 1] + v/2\n",
    "norm = np.sqrt(u**2+v**2) \n",
    "\n",
    "\n",
    "plt.plot(reduced_data[:-num_clusters, 0], reduced_data[:-num_clusters, 1],linewidth=0.5)\n",
    "sns.scatterplot(x=reduced_data[:-num_clusters, 0], y=reduced_data[:-num_clusters, 1], size=10)#,hue=cluster_labels,palette='Set1')\n",
    "\n",
    "\n",
    "\n",
    "plt.quiver(pos_x, pos_y, u/norm, v/norm, angles=\"xy\", zorder=5, pivot=\"mid\",width=0.001,headwidth=20)\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "'''centroids = reduced_data[-num_clusters:]\n",
    "sns.scatterplot(\n",
    "    x=centroids[:, 0],\n",
    "    y=centroids[:, 1],\n",
    "    marker=\"x\",\n",
    "    s=600,\n",
    "    hue=np.linspace(0,num_clusters-1,num_clusters),\n",
    "    palette='Set1',\n",
    "    legend=False\n",
    ")'''\n",
    "\n",
    "\n",
    "plt.xlim(x_min-7, x_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.legend('',frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords=np.array([[-13.5,0.4],\n",
    "        [35,0.4],\n",
    "        [7,-6],\n",
    "        [7,7]])\n",
    "for coord in coords : \n",
    "    plt.figure()\n",
    "    plt.imshow(pca_2d_temp.inverse_transform(coord).reshape(76,76),cmap='viridis')\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 7.5))\n",
    "\n",
    "components = pca_2d_temp.components_\n",
    "for c, comp in enumerate(components):\n",
    "    im=axs[c].imshow(comp.reshape(int(np.sqrt(len(comp))), int(np.sqrt(len(comp)))), cmap='bwr')\n",
    "    axs[c].axis('off')  # Remove axis labels for cleaner visualization\n",
    "    axs[c].set_title(f'PC {c+1}')\n",
    "\n",
    "\n",
    "# Place colorbar on the right side vertically\n",
    "cbar_ax = fig.add_axes([0.92, 0.255, 0.02, 0.47])\n",
    "\n",
    "fig.colorbar(im, cax=cbar_ax, orientation='vertical', cmap='bwr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time = np.linspace(1, 2400, len(correlation_matrices_temp))\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange', 'black', 'purple']\n",
    "\n",
    "plt.figure(figsize=(16, 8))  # Increase the figure size\n",
    "\n",
    "for a in range(num_clusters):\n",
    "    plt.plot(time, distances_temp[a, :], label=f'State {str(a)}', c=colors[a])\n",
    "\n",
    "plt.xlabel('Time [s]', fontsize=24)\n",
    "plt.ylabel('1- relative distance', fontsize=24)\n",
    "plt.legend(fontsize=20)\n",
    "plt.tick_params(axis='y', labelsize=18)\n",
    "plt.tick_params(axis='x', labelsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results dif noise, G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TVB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = f'D:/Fariba/5min_surface/'\n",
    "for i in [2]:#[2,5,7,8,9,10,11,12]:\n",
    "    subj = 'TVB'+str(i)\n",
    "    path_data_eeg =  f'{path_conn}/{subj}/'\n",
    "    data = np.load(f'{path_data_eeg}/segment29.npz')\n",
    "    savg_time = data['savg_time']\n",
    "    savg_data = data['savg_data']\n",
    "    teeg_time = data['eeg_time']\n",
    "    teeg_data = data['eeg_data']\n",
    "    \n",
    "    # desired ouput \n",
    "    savg_y1y2 = savg_data[:, 1, :, 0]-savg_data[:, 2, :, 0]\n",
    "    teeg_y1y2 = (teeg_data[:, 1, :, 0]-teeg_data[:, 2, :, 0])\n",
    "    \n",
    "    t1=1000\n",
    "    t2=300000\n",
    "\n",
    "    #source activity\n",
    "    id = np.where((savg_time>=t1)*(savg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    source_data = savg_y1y2[idt1:idt2]\n",
    "\n",
    "    #eeg\n",
    "    id = np.where((teeg_time>=t1)*(teeg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    eeg_data = teeg_y1y2[idt1:idt2]\n",
    "    \n",
    "    print(np.array(source_data).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats2,most_repeating_states2, correlation_matrices2, cluster_labels2,distances2, pca2, reduced_data2 = DFC_visu_pipeline(source_data,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TVB5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = f'D:/Fariba/5min_surface/'\n",
    "for i in [5]:#[2,5,7,8,9,10,11,12]:\n",
    "    subj = 'TVB'+str(i)\n",
    "    path_data_eeg =  f'{path_conn}/{subj}/'\n",
    "    data = np.load(f'{path_data_eeg}/segment29.npz')\n",
    "    savg_time = data['savg_time']\n",
    "    savg_data = data['savg_data']\n",
    "    teeg_time = data['eeg_time']\n",
    "    teeg_data = data['eeg_data']\n",
    "    \n",
    "    # desired ouput \n",
    "    savg_y1y2 = savg_data[:, 1, :, 0]-savg_data[:, 2, :, 0]\n",
    "    teeg_y1y2 = (teeg_data[:, 1, :, 0]-teeg_data[:, 2, :, 0])\n",
    "    \n",
    "    t1=1000\n",
    "    t2=300000\n",
    "\n",
    "    #source activity\n",
    "    id = np.where((savg_time>=t1)*(savg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    source_data = savg_y1y2[idt1:idt2]\n",
    "\n",
    "    #eeg\n",
    "    id = np.where((teeg_time>=t1)*(teeg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    eeg_data = teeg_y1y2[idt1:idt2]\n",
    "    \n",
    "    print(np.array(source_data).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats5,most_repeating_states5, correlation_matrices5, cluster_labels5,distances5, pca5, reduced_data5 = DFC_visu_pipeline(source_data,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_PCs(pca5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TVB7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = f'D:/Fariba/15Hz_2/'\n",
    "for i in [7]:#[2,5,7,8,9,10,11,12]:\n",
    "    subj = 'TVB'+str(i)\n",
    "    path_data_eeg =  f'{path_conn}/{subj}/'\n",
    "    data = np.load(f'{path_data_eeg}/segment55.npz')\n",
    "    savg_time = data['savg_time']\n",
    "    savg_data = data['savg_data']\n",
    "    teeg_time = data['eeg_time']\n",
    "    teeg_data = data['eeg_data']\n",
    "    \n",
    "    # desired ouput \n",
    "    savg_y1y2 = savg_data[:, 1, :, 0]-savg_data[:, 2, :, 0]\n",
    "    teeg_y1y2 = (teeg_data[:, 1, :, 0]-teeg_data[:, 2, :, 0])\n",
    "    \n",
    "    t1=1000\n",
    "    t2=300000\n",
    "\n",
    "    #source activity\n",
    "    id = np.where((savg_time>=t1)*(savg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    source_data = savg_y1y2[idt1:idt2]\n",
    "\n",
    "    #eeg\n",
    "    id = np.where((teeg_time>=t1)*(teeg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    eeg_data = teeg_y1y2[idt1:idt2]\n",
    "    \n",
    "    print(np.array(source_data).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats7_2,most_repeating_states7_2, correlation_matrices7_2, cluster_labels7_2,distances7_2, pca7_2, reduced_data7_2 = DFC_visu_pipeline(source_data,meth,animated_visu=False, window_size=1000,overlap=950,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = f'D:/Fariba/5min_surface/'\n",
    "for i in [7]:#[2,5,7,8,9,10,11,12]:\n",
    "    subj = 'TVB'+str(i)\n",
    "    path_data_eeg =  f'{path_conn}/{subj}/'\n",
    "    data = np.load(f'{path_data_eeg}/segment29.npz')\n",
    "    savg_time = data['savg_time']\n",
    "    savg_data = data['savg_data']\n",
    "    teeg_time = data['eeg_time']\n",
    "    teeg_data = data['eeg_data']\n",
    "    \n",
    "    # desired ouput \n",
    "    savg_y1y2 = savg_data[:, 1, :, 0]-savg_data[:, 2, :, 0]\n",
    "    teeg_y1y2 = (teeg_data[:, 1, :, 0]-teeg_data[:, 2, :, 0])\n",
    "    \n",
    "    t1=1000\n",
    "    t2=300000\n",
    "\n",
    "    #source activity\n",
    "    id = np.where((savg_time>=t1)*(savg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    source_data = savg_y1y2[idt1:idt2]\n",
    "\n",
    "    #eeg\n",
    "    id = np.where((teeg_time>=t1)*(teeg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    eeg_data = teeg_y1y2[idt1:idt2]\n",
    "    \n",
    "    print(np.array(source_data).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods :\n",
    "        closest_mats7,most_repeating_states7, correlation_matrices7, cluster_labels7,distances7, pca7, reduced_data7 = DFC_visu_pipeline(source_data,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TVB8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = f'D:/Fariba/5min_surface/'\n",
    "for i in [8]:#[2,5,7,8,9,10,11,12]:\n",
    "    subj = 'TVB'+str(i)\n",
    "    path_data_eeg =  f'{path_conn}/{subj}/'\n",
    "    data = np.load(f'{path_data_eeg}/segment29.npz')\n",
    "    savg_time = data['savg_time']\n",
    "    savg_data = data['savg_data']\n",
    "    teeg_time = data['eeg_time']\n",
    "    teeg_data = data['eeg_data']\n",
    "    \n",
    "    # desired ouput \n",
    "    savg_y1y2 = savg_data[:, 1, :, 0]-savg_data[:, 2, :, 0]\n",
    "    teeg_y1y2 = (teeg_data[:, 1, :, 0]-teeg_data[:, 2, :, 0])\n",
    "    \n",
    "    t1=1000\n",
    "    t2=300000\n",
    "\n",
    "    #source activity\n",
    "    id = np.where((savg_time>=t1)*(savg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    source_data = savg_y1y2[idt1:idt2]\n",
    "\n",
    "    #eeg\n",
    "    id = np.where((teeg_time>=t1)*(teeg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    eeg_data = teeg_y1y2[idt1:idt2]\n",
    "    \n",
    "    print(np.array(source_data).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats8,most_repeating_states8, correlation_matrices8, cluster_labels8,distances8, pca8, reduced_data8 = DFC_visu_pipeline(source_data,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TVB9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = f'D:/Fariba/5min_surface/'\n",
    "for i in [9]:#[2,5,7,8,9,10,11,12]:\n",
    "    subj = 'TVB'+str(i)\n",
    "    path_data_eeg =  f'{path_conn}/{subj}/'\n",
    "    data = np.load(f'{path_data_eeg}/segment29.npz')\n",
    "    savg_time = data['savg_time']\n",
    "    savg_data = data['savg_data']\n",
    "    teeg_time = data['eeg_time']\n",
    "    teeg_data = data['eeg_data']\n",
    "    \n",
    "    # desired ouput \n",
    "    savg_y1y2 = savg_data[:, 1, :, 0]-savg_data[:, 2, :, 0]\n",
    "    teeg_y1y2 = (teeg_data[:, 1, :, 0]-teeg_data[:, 2, :, 0])\n",
    "    \n",
    "    t1=1000\n",
    "    t2=300000\n",
    "\n",
    "    #source activity\n",
    "    id = np.where((savg_time>=t1)*(savg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    source_data = savg_y1y2[idt1:idt2]\n",
    "\n",
    "    #eeg\n",
    "    id = np.where((teeg_time>=t1)*(teeg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    eeg_data = teeg_y1y2[idt1:idt2]\n",
    "    \n",
    "    print(np.array(source_data).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats9,most_repeating_states9, correlation_matrices9, cluster_labels9,distances9, pca9, reduced_data9 = DFC_visu_pipeline(source_data,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrahemispheric eeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=f'D:/Fariba/5min_surface/'\n",
    "Subjects=['TVB2','TVB5','TVB7','TVB8','TVB9']#,'TVB10','TVB11','TVB12']\n",
    "right_indices=[]\n",
    "left_indices=[]\n",
    "middle_indices=[]\n",
    "ordered_ind=[]\n",
    "for i, subj in enumerate (Subjects) : \n",
    "    conn_file=file_path+subj+'/eeg_1010.txt'\n",
    "    r_ind=[]\n",
    "    l_ind=[]\n",
    "    m_ind=[]\n",
    "    with open(conn_file) as f:\n",
    "        lines = f.readlines()\n",
    "    for j, line in enumerate(lines) : \n",
    "        words = line.split()\n",
    "        # Get the first word (element before the first space)\n",
    "        first_word = words[0]\n",
    "        print(first_word)\n",
    "        if first_word[-1].isdigit() : \n",
    "            last=int(first_word[-1])\n",
    "            if last%2==0 : \n",
    "                r_ind.append(j)\n",
    "                print('right')\n",
    "            elif last%2==1 : \n",
    "                l_ind.append(j)\n",
    "                print('left')\n",
    "        else :\n",
    "                m_ind.append(j)\n",
    "                print('middle')    \n",
    "\n",
    "    right_indices.append(r_ind)\n",
    "    left_indices.append(l_ind)\n",
    "    middle_indices.append(m_ind)\n",
    "    ordered_ind.append(np.concatenate((l_ind,m_ind,r_ind)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_ind=np.concatenate((left_indices[1],middle_indices[1],right_indices[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TVB2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all subjects eeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = f'D:/Fariba/5min_surface/'\n",
    "closest_mats_joined=[]\n",
    "for i in [2,5,7,8,9]:#[2,5,7,8,9,10,11,12]:\n",
    "    subj = 'TVB'+str(i)\n",
    "    path_data_eeg =  f'{path_conn}/{subj}/'\n",
    "    data = np.load(f'{path_data_eeg}/segment29.npz')\n",
    "    savg_time = data['savg_time']\n",
    "    savg_data = data['savg_data']\n",
    "    teeg_time = data['eeg_time']\n",
    "    teeg_data = data['eeg_data']\n",
    "    \n",
    "    # desired ouput \n",
    "    savg_y1y2 = savg_data[:, 1, :, 0]-savg_data[:, 2, :, 0]\n",
    "    teeg_y1y2 = (teeg_data[:, 1, :, 0]-teeg_data[:, 2, :, 0])\n",
    "    \n",
    "    t1=1000\n",
    "    t2=300000\n",
    "\n",
    "    #source activity\n",
    "    id = np.where((savg_time>=t1)*(savg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    source_data = savg_y1y2[idt1:idt2]\n",
    "\n",
    "    #eeg\n",
    "    id = np.where((teeg_time>=t1)*(teeg_time<=t2))\n",
    "    idt1 = id[0][0]\n",
    "    idt2 = id[0][-1]\n",
    "    \n",
    "    eeg_data = teeg_y1y2[idt1:idt2]\n",
    "    \n",
    "    print(np.array(source_data).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(eeg_data,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False)\n",
    "    closest_mats_joined.append(closest_mats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, subj in enumerate([2,5,7,8,9]):\n",
    "    plt.figure()\n",
    "    plt.imshow(closest_mats_joined[i][0][ordered_ind][ordered_ind],cmap='viridis')\n",
    "    plt.title(f'Subject {subj}')\n",
    "    plt.colorbar()\n",
    "    plt.clim([0,1])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_mats_joined[i][0][ordered_ind].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_closest_mats_joined = []\n",
    "for i,closest_mats in enumerate(closest_mats_joined):\n",
    "    ordered_closest_mats = []\n",
    "    for matrix in closest_mats:\n",
    "        ordered_matrix = matrix[ordered_ind[i]][:, ordered_ind[i]]\n",
    "        ordered_closest_mats.append(ordered_matrix)\n",
    "    ordered_closest_mats_joined.append(ordered_closest_mats)\n",
    "\n",
    "for i, subj in enumerate([2,5,7,8,9]):\n",
    "    for j in range(3):\n",
    "        plt.figure()\n",
    "        plt.imshow(ordered_closest_mats_joined[i][j],cmap='viridis')\n",
    "        plt.title(f'Subject {subj}')\n",
    "        plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_mats_joined = np.array(closest_mats_joined)\n",
    "closest_mats_joined.shape\n",
    "for i in closest_mats_joined:\n",
    "    print(i.shape)\n",
    "    for j in i: \n",
    "        print(j.shape) \n",
    "        print(j[ordered_ind][:, ordered_ind].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_closest_mats_joined = np.array(ordered_closest_mats_joined)\n",
    "ordered_closest_mats_joined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_ind[i].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New plan projection Esra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TVB10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/40sim_dt_0.5_G_{7}_sigma_{1e-9}_TVB10.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 300000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca10, reduced_data = DFC_visu_pipeline(data_corr,meth,animated_visu=False, window_size=1000,overlap=500,tmax=t2/1000, intrahemispheric=False, square=[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca10.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_PC1 = components[0]\n",
    "new_PC2 = components[1]-components[2]\n",
    "new_PC3 = components[1]+components[2]\n",
    "new_PCs=np.array([new_PC1,new_PC2,new_PC3])\n",
    "\n",
    "for PC in new_PCs : \n",
    "    plt.figure()\n",
    "    plt.imshow(PC.reshape(84,84),cmap='bwr')\n",
    "    plt.colorbar();plt.clim([-0.05,0.05])\n",
    "\n",
    "new_PCs=pca10.transform(new_PCs)\n",
    "new_PC1=new_PCs[0]\n",
    "new_PC2=new_PCs[1]\n",
    "new_PC3=new_PCs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_plane(point1, point2):\n",
    "    # Calculate the normal vector of the plane\n",
    "    normal = np.cross(point1, point2)\n",
    "    return normal\n",
    "\n",
    "# Function to project points onto the plane\n",
    "def project_onto_plane(plane_normal, plane_point, other_points):\n",
    "    # Ensure the plane normal is a unit vector\n",
    "    plane_normal = plane_normal / np.linalg.norm(plane_normal)\n",
    "    \n",
    "    # Calculate the equation of the plane (ax + by + cz = d)\n",
    "    d = np.dot(plane_normal, plane_point)\n",
    "    \n",
    "    # Calculate the projection of other points onto the plane\n",
    "    projections = []\n",
    "    for point in other_points:\n",
    "        t = d - np.dot(plane_normal, point)\n",
    "        projected_point = point + t * plane_normal\n",
    "        projections.append(projected_point)\n",
    "    \n",
    "    return projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan1=define_plane(new_PC1,new_PC2)\n",
    "plan2=define_plane(new_PC1,new_PC3)\n",
    "plan3=define_plane(new_PC2,new_PC3)\n",
    "\n",
    "proj1=np.array(project_onto_plane(plan1, new_PC1, reduced_data))\n",
    "proj2=np.array(project_onto_plane(plan2, new_PC1, reduced_data))\n",
    "proj3=np.array(project_onto_plane(plan3, new_PC2, reduced_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLAN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter3d(x=proj1[:-num_clusters, 0], y=proj1[:-num_clusters, 1], z=proj1[:-num_clusters, 2],\n",
    "                                    mode='markers', marker=dict(color=cluster_labels))])\n",
    "\n",
    "centroids = proj1[-num_clusters:]\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=centroids[:, 0], y=centroids[:, 1], z=centroids[:, 2],\n",
    "                                mode='markers+text', marker=dict(symbol='x', size=6),\n",
    "                            text=[str(i) for i in range(num_clusters)], name='Centroids', showlegend=False))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=proj1[:-num_clusters, 0],y=proj1[:-num_clusters, 1],z=proj1[:-num_clusters, 2],mode='lines',line=dict(color='black', width=2)))\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"Projected PCA; axis1=PC1/ axis2=PC2-PC3\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_3d_to_2d(points_3d, plane_normal):\n",
    "    # Ensure the plane normal is a unit vector\n",
    "    plane_normal = plane_normal / np.linalg.norm(plane_normal)\n",
    "    \n",
    "    # Calculate the projection of points onto the plane\n",
    "    # The projection formula: P = V - dot(V, N) * N, where V is the point and N is the normal vector\n",
    "    projected_points = points_3d - np.dot(points_3d, plane_normal)[:, np.newaxis] * plane_normal\n",
    "    \n",
    "    return projected_points\n",
    "\n",
    "proj1_2d=pd.DataFrame(project_3d_to_2d(proj1,np.cross(proj1[0],proj1[1])))\n",
    "\n",
    "plt.scatter(proj1_2d[0][:-num_clusters],proj1_2d[1][:-num_clusters],c=cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorscale = ['#7A4579', '#D56073', 'rgb(236,158,105)', (1, 1, 0.2), (0.98,0.98,0.98)]\n",
    "\n",
    "fig = ff.create_2d_density(\n",
    "    proj1_2d[0][:-num_clusters],proj1_2d[1][:-num_clusters], colorscale=colorscale,\n",
    "    hist_color='rgb(255, 237, 222)', point_size=3\n",
    ")\n",
    "fig.update_layout(title=f\"Desity plot of projected points\", \n",
    "    height=1000, width=1000)\n",
    "\n",
    "# Add crosses marking the projections of the centroids\n",
    "centroids_proj = project_3d_to_2d(centroids, np.cross(proj1[0], proj1[1]))\n",
    "fig.add_trace(go.Scatter(x=centroids_proj[:, 0], y=centroids_proj[:, 1],\n",
    "                         mode='markers+text', marker=dict(symbol='x', size=20,color='white'),text=[str(i) for i in range(num_clusters)],\n",
    "                         \n",
    "                         name='Centroids'))\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"Desity plot of projected points\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_center=[-0.5,0.5]\n",
    "xs=proj1_2d[:-num_clusters][0]\n",
    "ys=proj1_2d[:-num_clusters][1]\n",
    "\n",
    "dx=manual_center[0]-xs\n",
    "dy=manual_center[1]-ys\n",
    "\n",
    "radial_distance=np.sqrt(dx**2+dy**2)\n",
    "angles=np.arctan2(dy,dx)\n",
    "unwraped=np.unwrap(angles)\n",
    "indeg=np.degrees(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=go.Figure(data=[go.Scatter(x=list(range(len(unwraped))),y=unwraped,mode='markers',marker=dict(color=radial_distance,size=5,showscale=True))])\n",
    "fig.update_layout(coloraxis=dict(colorbar=dict(title='Radial Distance')))\n",
    "fig.update_layout(title=f\"Angle evolution, Subj10, Healthy\", \n",
    "    height=1000, width=1750)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLAN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter3d(x=proj2[:-num_clusters, 0], y=proj2[:-num_clusters, 1], z=proj2[:-num_clusters, 2],\n",
    "                                    mode='markers', marker=dict(color=cluster_labels))])\n",
    "\n",
    "centroids = proj2[-num_clusters:]\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=centroids[:, 0], y=centroids[:, 1], z=centroids[:, 2],\n",
    "                                mode='markers+text', marker=dict(symbol='x', size=6),\n",
    "                            text=[str(i) for i in range(num_clusters)], name='Centroids', showlegend=False))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=proj2[:-num_clusters, 0],y=proj2[:-num_clusters, 1],z=proj2[:-num_clusters, 2],mode='lines',line=dict(color='black', width=2)))\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"Projected PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Centroids are marked with cross\\n\"\n",
    "    f\"points are linked according to temporal order\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj2_2d=pd.DataFrame(project_3d_to_2d(proj2,np.cross(proj2[0],proj2[4])))\n",
    "\n",
    "plt.scatter(proj2_2d[0][:-num_clusters],proj2_2d[1][:-num_clusters],c=cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLAN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter3d(x=proj3[:-num_clusters, 0], y=proj3[:-num_clusters, 1], z=proj3[:-num_clusters, 2],\n",
    "                                    mode='markers', marker=dict(color=cluster_labels))])\n",
    "\n",
    "centroids = proj3[-num_clusters:]\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=centroids[:, 0], y=centroids[:, 1], z=centroids[:, 2],\n",
    "                                mode='markers+text', marker=dict(symbol='x', size=6),\n",
    "                            text=[str(i) for i in range(num_clusters)], name='Centroids', showlegend=False))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=proj3[:-num_clusters, 0],y=proj3[:-num_clusters, 1],z=proj3[:-num_clusters, 2],mode='lines',line=dict(color='black', width=2)))\n",
    "\n",
    "\n",
    "fig.update_layout(title=f\"Projected PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Centroids are marked with cross\\n\"\n",
    "    f\"points are linked according to temporal order\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(proj3[:-num_clusters, 2],proj3[:-num_clusters, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorscale = ['#7A4579', '#D56073', 'rgb(236,158,105)', (1, 1, 0.2), (0.98,0.98,0.98)]\n",
    "\n",
    "fig = ff.create_2d_density(\n",
    "    proj3[:-num_clusters, 1], proj3[:-num_clusters, 2], colorscale=colorscale,\n",
    "    hist_color='rgb(255, 237, 222)', point_size=3\n",
    ")\n",
    "plt.scatter()\n",
    "fig.update_layout(title=f\"Projected PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Centroids are marked with cross\\n\"\n",
    "    f\"points are linked according to temporal order\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con1=proj3[:, 1]/proj3[:, 0]\n",
    "con2=proj3[:, 2]/proj3[:, 0]\n",
    "con=np.array([con1,con2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(con[0,:-num_clusters],con[1,:-num_clusters],c=cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/20minsim_dt_0.5_G_{7}_sigma_1e-07.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 2400000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=10000,overlap=5000,tmax=t2/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_data=np.concatenate((correlation_matrices,most_repeating_states))\n",
    "data=np.array(ext_data).reshape(len(correlation_matrices)+num_clusters,len(correlation_matrices[0])**2)\n",
    "reduced_data_2d = PCA(n_components=2).fit_transform(data)\n",
    "manual_center=[6,0]\n",
    "\n",
    "colorscale = ['#7A4579', '#D56073', 'rgb(236,158,105)', (1, 1, 0.2), (0.98,0.98,0.98)]\n",
    "\n",
    "fig = ff.create_2d_density(\n",
    "    reduced_data_2d[:-num_clusters, 0], reduced_data_2d[:-num_clusters, 1], colorscale=colorscale,\n",
    "    hist_color='rgb(255, 237, 222)', point_size=3\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=[manual_center[0]],y=[manual_center[1]],mode='markers',marker=dict(symbol='x',color='black',size=10)))\n",
    "\n",
    "fig.update_layout(title=f\"Projected PCA-reduced connectivity matrices afetr K-means clustering  \\n\"\n",
    "    \"Centroids are marked with cross\\n\"\n",
    "    f\"points are linked according to temporal order\", \n",
    "    height=1000, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angle and radial distance evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_center=[6,0]\n",
    "xs=reduced_data_2d[:-num_clusters, 0]\n",
    "ys=reduced_data_2d[:-num_clusters,1]\n",
    "\n",
    "dx=manual_center[0]-xs\n",
    "dy=manual_center[1]-ys\n",
    "\n",
    "radial_distance=np.sqrt(dx**2+dy**2)\n",
    "angles=np.arctan2(dy,dx)\n",
    "unwraped=np.unwrap(angles)\n",
    "indeg=np.degrees(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(indeg)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('angle')\n",
    "plt.title('Angle of each point with respect to the ellipse center')\n",
    "plt.figure()\n",
    "plt.plot(radial_distance)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('radial distance')\n",
    "plt.title('Radial distance of each point accross time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(unwraped)\n",
    "plt.ylabel('unwraped angle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=go.Figure(data=[go.Scatter(x=list(range(len(unwraped))),y=unwraped,mode='markers',marker=dict(color=radial_distance,size=5,showscale=True))])\n",
    "fig.update_layout(coloraxis=dict(colorbar=dict(title='Radial Distance')))\n",
    "fig.update_layout(title=f\"Angle evolution\", \n",
    "    height=1000, width=1750)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(radial_distance,indeg)\n",
    "plt.title('redial distance vs angle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corr corr Matrix + dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_corr_mat=np.corrcoef(np.array(correlation_matrices).reshape(len(correlation_matrices),len(correlation_matrices[0])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(corr_corr_mat)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_data = linkage(corr_corr_mat, method='ward', metric='euclidean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "k=8\n",
    "dend=dendrogram(linkage_data, orientation='top', color_threshold=8)  \n",
    "clusters=scipy.cluster.hierarchy.fcluster(linkage_data, 8, criterion='maxclust')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "dend=dendrogram(linkage_data, orientation='top', truncate_mode='lastp', p=8, show_leaf_counts=True, show_contracted=True, no_labels=True, color_threshold=100)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=ff.create_dendrogram(corr_corr_mat, color_threshold=4.5)\n",
    "fig.update_layout(width=800, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testi=scipy.cluster.hierarchy.fcluster(linkage_data, 8, criterion='maxclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust1=[]\n",
    "clust2=[]\n",
    "clust3=[]\n",
    "clust4=[]\n",
    "clust5=[]\n",
    "clust6=[]\n",
    "clust7=[]\n",
    "clust8=[]\n",
    "\n",
    "clusts=[ [] for _ in range(8) ]\n",
    "\n",
    "for i in range(len(correlation_matrices)):\n",
    "    clusts[clusters[i]-1].append(correlation_matrices[i])\n",
    "\n",
    "for j in range(len(clusts)):\n",
    "    plt.figure()\n",
    "    plt.imshow(np.mean(clusts[j],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorg_matrices = [clusts[0]+clusts[1]+clusts[2]+clusts[3]+clusts[4]+clusts[5]+clusts[6]+clusts[7]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust1=np.zeros(len(testi[testi==1]))\n",
    "clust2=np.zeros(len(testi[testi==2]))\n",
    "clust3=np.zeros(len(testi[testi==3]))\n",
    "clust4=np.zeros(len(testi[testi==4]))\n",
    "clust5=np.zeros(len(testi[testi==5]))\n",
    "clust6=np.zeros(len(testi[testi==6]))\n",
    "clust7=np.zeros(len(testi[testi==7]))\n",
    "clust8=np.zeros(len(testi[testi==8]))\n",
    "\n",
    "for i in range(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "# get data\n",
    "data = np.genfromtxt(\"http://files.figshare.com/2133304/ExpRawData_E_TABM_84_A_AFFY_44.tab\",\n",
    "                     names=True,usecols=tuple(range(1,30)),dtype=float, delimiter=\"\\t\")\n",
    "data_array = data.view((float, len(data.dtype.names)))\n",
    "data_array = corr_corr_mat\n",
    "\n",
    "\n",
    "# Initialize figure by creating upper dendrogram\n",
    "fig = ff.create_dendrogram(data_array, orientation='bottom')\n",
    "for i in range(len(fig['data'])):\n",
    "    fig['data'][i]['yaxis'] = 'y2'\n",
    "\n",
    "# Create Side Dendrogram\n",
    "dendro_side = ff.create_dendrogram(data_array, orientation='right')\n",
    "for i in range(len(dendro_side['data'])):\n",
    "    dendro_side['data'][i]['xaxis'] = 'x2'\n",
    "\n",
    "# Add Side Dendrogram Data to Figure\n",
    "for data in dendro_side['data']:\n",
    "    fig.add_trace(data)\n",
    "\n",
    "# Create Heatmap\n",
    "dendro_leaves = dendro_side['layout']['yaxis']['ticktext']\n",
    "dendro_leaves = list(map(int, dendro_leaves))\n",
    "data_dist = pdist(data_array)\n",
    "heat_data = squareform(data_dist)\n",
    "heat_data = heat_data[dendro_leaves,:]\n",
    "heat_data = heat_data[:,dendro_leaves]\n",
    "\n",
    "heatmap = [\n",
    "    go.Heatmap(\n",
    "        x = dendro_leaves,\n",
    "        y = dendro_leaves,\n",
    "        z = heat_data,\n",
    "        colorscale = 'Blues'\n",
    "    )\n",
    "]\n",
    "\n",
    "heatmap[0]['x'] = fig['layout']['xaxis']['tickvals']\n",
    "heatmap[0]['y'] = dendro_side['layout']['yaxis']['tickvals']\n",
    "\n",
    "# Add Heatmap Data to Figure\n",
    "for data in heatmap:\n",
    "    fig.add_trace(data)\n",
    "\n",
    "# Edit Layout\n",
    "fig.update_layout({'width':2000, 'height':2000,\n",
    "                         'showlegend':False, 'hovermode': 'closest',\n",
    "                         })\n",
    "# Edit xaxis\n",
    "fig.update_layout(xaxis={'domain': [.15, 1],\n",
    "                                  'mirror': False,\n",
    "                                  'showgrid': False,\n",
    "                                  'showline': False,\n",
    "                                  'zeroline': False,\n",
    "                                  'ticks':\"\"})\n",
    "# Edit xaxis2\n",
    "fig.update_layout(xaxis2={'domain': [0, .15],\n",
    "                                   'mirror': False,\n",
    "                                   'showgrid': False,\n",
    "                                   'showline': False,\n",
    "                                   'zeroline': False,\n",
    "                                   'showticklabels': False,\n",
    "                                   'ticks':\"\"})\n",
    "\n",
    "# Edit yaxis\n",
    "fig.update_layout(yaxis={'domain': [0, .85],\n",
    "                                  'mirror': False,\n",
    "                                  'showgrid': False,\n",
    "                                  'showline': False,\n",
    "                                  'zeroline': False,\n",
    "                                  'showticklabels': False,\n",
    "                                  'ticks': \"\"\n",
    "                        })\n",
    "# Edit yaxis2\n",
    "fig.update_layout(yaxis2={'domain':[.825, .975],\n",
    "                                   'mirror': False,\n",
    "                                   'showgrid': False,\n",
    "                                   'showline': False,\n",
    "                                   'zeroline': False,\n",
    "                                   'showticklabels': False,\n",
    "                                   'ticks':\"\"})\n",
    "\n",
    "# Plot!\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timesteps investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestep 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conn = 'D:/Fariba/SC'\n",
    "path_save = f'D:/Timing_optimisation'\n",
    "data = np.load(f'{path_save}/20minsim_dt_0.5_G_{7}_sigma_1e-07.npz')\n",
    "data_time=data['traw_time']\n",
    "delta_t = data_time[1]-data_time[0]\n",
    "sf = 1/(delta_t*1e-3)\n",
    "win = 1*sf\n",
    "\n",
    "t1 = 1000\n",
    "t2 = 600000 #unit second\n",
    "time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "idc1 = time_corr_id[0][0]\n",
    "idc2= time_corr_id[0][-1]\n",
    "time_corr=data_time[idc1:idc2]\n",
    "data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "print(np.array(data_corr).shape)\n",
    "\n",
    "methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "for meth in methods : \n",
    "    closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=10000,overlap=5000,tmax=t2/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestep 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [40,50,60]\n",
    "for s in seeds :\n",
    "    path_conn = 'D:/Fariba/SC'\n",
    "    path_save = f'D:/Fariba/Timestep_investigation'\n",
    "    data = np.load(f'{path_save}/5minsim_dt_0.3_G_{7}_sigma_1e-07_seed{s}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 600000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=int(10000*0.5/0.3),overlap=int(5000*0.5/0.3),tmax=t2/1000,animated_visu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestep 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [40,50,60]\n",
    "for s in seeds :\n",
    "    path_conn = 'D:/Fariba/SC'\n",
    "    path_save = f'D:/Fariba/Timestep_investigation'\n",
    "    data = np.load(f'{path_save}/5minsim_dt_0.2_G_{7}_sigma_1e-07_seed{s}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 600000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=int(10000*0.5/0.2),overlap=int(5000*0.5/0.2),tmax=t2/1000,animated_visu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestep 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [40,50,60]\n",
    "for s in seeds :\n",
    "    path_conn = 'D:/Fariba/SC'\n",
    "    path_save = f'D:/Fariba/Timestep_investigation'\n",
    "    data = np.load(f'{path_save}/5minsim_dt_0.1_G_{7}_sigma_1e-07_seed{s}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 600000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=int(10000*0.5/0.1),overlap=int(5000*0.5/0.1),tmax=t2/1000,animated_visu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestep 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [40,50,60]\n",
    "for s in seeds :\n",
    "    path_conn = 'D:/Fariba/SC'\n",
    "    path_save = f'D:/Fariba/Timestep_investigation'\n",
    "    data = np.load(f'{path_save}/5minsim_dt_0.05_G_{7}_sigma_1e-07_seed{s}.npz')\n",
    "    data_time=data['traw_time']\n",
    "    delta_t = data_time[1]-data_time[0]\n",
    "    sf = 1/(delta_t*1e-3)\n",
    "    win = 1*sf\n",
    "\n",
    "    t1 = 1000\n",
    "    t2 = 600000 #unit second\n",
    "    time_corr_id= np.where((data_time>=t1)*(data_time<=t2))\n",
    "    idc1 = time_corr_id[0][0]\n",
    "    idc2= time_corr_id[0][-1]\n",
    "    time_corr=data_time[idc1:idc2]\n",
    "    data_corr=data['traw_y1y2'][idc1:idc2]\n",
    "    print(np.array(data_corr).shape)\n",
    "\n",
    "    methods=['Pearson']#,'Spearman', 'Coherence', 'Phase-lock' ] #'Pearson', 'Coherence','Cross-cor'\n",
    "    for meth in methods : \n",
    "        closest_mats,most_repeating_states, correlation_matrices, cluster_labels,distances, pca, reduced_data = DFC_visu_pipeline(data_corr,meth, window_size=int(10000*0.5/0.05),overlap=int(5000*0.5/0.05),tmax=t2/1000,animated_visu=False,ts=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestep 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
